{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### A plan how the essay scoring experiment will be done"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import scipy as scipy\n",
    "import stats as stats\n",
    "\n",
    "'''\n",
    "1- using the tfIDF vectroizer or the sbert pretrained model to vectroize the sentences into vectors and get the features vectors.\n",
    "\n",
    "2- then apped the clusters vectors an array of 19 elements to the end of the vectors array.\n",
    "\n",
    "\n",
    "3- train the model using of the shallow machine learning model.......\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets, svm, tree, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import sys\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.sparse import csr_matrix, vstack, hstack\n",
    "from scipy import sparse\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as pltb\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Building the right file names and reading the files"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\TRACE_Datensatz_transposed_220308.csv\", sep='\\t')\n",
    "y = df['Code1_IN_AR_Gesamteindruck_Argumentation_3_4_0']  # y is the gold standards\n",
    "\n",
    "listEssayNames = df['DocumentID']\n",
    "\n",
    "listEssayNames = [i.removeprefix(\"00\") for i in listEssayNames]\n",
    "\n",
    "listEssayNames = [i.replace(\"al\",\"aI\") for i in listEssayNames]\n",
    "\n",
    "listRightNames = []\n",
    "for essay in listEssayNames:\n",
    "    x = essay.split(\"X\")\n",
    "    temp = x[0]+\"_\"+x[1]+\".txt\"\n",
    "    listRightNames.append(temp)\n",
    "    #print(x)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "essays =[]\n",
    "df_fileName = pd.DataFrame({})\n",
    "for essayRigt in listRightNames:\n",
    "    my_file = Path(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\ALLEssaysAllPrompts\", essayRigt)\n",
    "    if my_file.is_file():\n",
    "        text = my_file.read_text(encoding='utf-8-sig')\n",
    "        essays.append(text)\n",
    "\n",
    "#df_fileName.to_excel(\"essaysToFileName.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Accuracy and Adjcent Accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def custom_adjacent_accuracy_score(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return np.sum(np.abs(y_pred - y_true) <= 1) / len(y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CountVectorizer Test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1716, 12399)\n",
      "(573, 12399)\n",
      "Accuracy: 0.3228621291448517\n",
      "ِAdjecent Accuracy: 0.7678883071553229\n",
      "[[ 0  0  3  3  2  0  0]\n",
      " [ 0 12 24 23  6  0  0]\n",
      " [ 0 22 44 48 30  5  0]\n",
      " [ 2 21 71 93 43  4  1]\n",
      " [ 0  4 17 39 33  4  1]\n",
      " [ 0  1  3  5  4  3  0]\n",
      " [ 0  0  0  0  2  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "essays_train, essays_test, y_train, y_test = train_test_split(essays, y, test_size=0.25, random_state=1000)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(essays_train)\n",
    "X_train = vectorizer.transform(essays_train)\n",
    "X_test  = vectorizer.transform(essays_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "classifier = LogisticRegression(max_iter=100)\n",
    "classifier.fit(X_train, y_train)\n",
    "score1 = classifier.score(X_test, y_test)\n",
    "pred = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", score1)\n",
    "print(\"ِAdjecent Accuracy:\",custom_adjacent_accuracy_score(y_test,pred))\n",
    "print(confusion_matrix(y_test, pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sbert Test With Clusters as Features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#essays_train, essays_test, y_train, y_test = train_test_split(essays, y, test_size=0.25, random_state=1000)\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device=\"cuda\")\n",
    "\n",
    "X_embeddings = model.encode(essays, show_progress_bar =True, device=\"cuda\")\n",
    "\n",
    "df_goldStandarsdClusters = pd.read_excel(\"FinalResults.xlsx\")\n",
    "\n",
    "clusterFeatureVectormulti = []\n",
    "finalX_Train = []\n",
    "\n",
    "for i in range (len(essays)):\n",
    "\n",
    "    clusterVectorFeatureSingle =  [0 for col in range(19)]\n",
    "\n",
    "    listEssayClusers = df_goldStandarsdClusters[df_goldStandarsdClusters['fileName'].str.contains(listRightNames[i])]['cluster']\n",
    "\n",
    "    if (len(listEssayClusers) ==0):\n",
    "        print(\"error\")\n",
    "\n",
    "    for cluster in listEssayClusers:\n",
    "        try:\n",
    "            clusterVectorFeatureSingle[cluster] = 1\n",
    "        except:\n",
    "            print(cluster)\n",
    "\n",
    "    clusterFeatureVectormulti.append(clusterVectorFeatureSingle)\n",
    "\n",
    "for i in range(len(X_embeddings)):\n",
    "    arr = np.append(X_embeddings[i],clusterFeatureVectormulti[i])\n",
    "    finalX_Train.append(arr)\n",
    "\n",
    "npa = np.asarray(finalX_Train, dtype=np.ndarray)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(npa, y, test_size=0.25, random_state=1000)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "classifier = SVC(kernel=\"linear\", C=0.025) #LogisticRegression()  #DecisionTreeClassifier(max_depth=4)\n",
    "classifier.fit(X_train, y_train)\n",
    "score1 = classifier.score(X_test, y_test)\n",
    "pred = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", score1)\n",
    "\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(\"QWK: \", cohen_kappa_score(y_test,pred,weights=\"quadratic\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sbert test without clusters as Features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/54 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53e548d011594edeb015043ca4d4f568"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/18 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7caaff84660b443bb4d37231d8dc64cb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1716, 768)\n",
      "(573, 768)\n",
      "Accuracy: 0.41012216404886565\n",
      "ِAdjecent Accuracy: 0.8394415357766143\n",
      "pearson correaltion 1  (0.2260998151449603, 4.4640303139982e-08)\n",
      "pearson correaltion 2 (0.2260998151449603, 4.4640303139982e-08)\n",
      "[[  0   0   4   3   1   0   0]\n",
      " [  0   0  19  46   0   0   0]\n",
      " [  0   0  34 103  12   0   0]\n",
      " [  0   1  31 183  20   0   0]\n",
      " [  0   0   9  71  18   0   0]\n",
      " [  0   0   0  14   2   0   0]\n",
      " [  0   0   0   2   0   0   0]]\n",
      "QWK:  0.1740535968522976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\AppData\\Local\\Temp\\ipykernel_17008\\244250591.py:20: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  print(\"pearson correaltion 2\", stats.pearsonr(y_test, pred))\n"
     ]
    }
   ],
   "source": [
    "essays_train, essays_test, y_train, y_test = train_test_split(essays, y, test_size=0.25, random_state=1000)\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device=\"cuda\")\n",
    "\n",
    "X_train = model.encode(essays_train, show_progress_bar =True, device=\"cuda\")\n",
    "\n",
    "X_test  = model.encode(essays_test, show_progress_bar =True, device=\"cuda\")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "classifier = LogisticRegression() #SVC(kernel=\"linear\", C=0.025) #LogisticRegression()  #DecisionTreeClassifier(max_depth=4)\n",
    "classifier.fit(X_train, y_train)\n",
    "score1 = classifier.score(X_test, y_test)\n",
    "pred = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", score1)\n",
    "print(\"ِAdjecent Accuracy:\",custom_adjacent_accuracy_score(y_test,pred))\n",
    "\n",
    "print(\"pearson correaltion 1 \",scipy.stats.pearsonr(y_test, pred))\n",
    "print(\"pearson correaltion 2\", stats.pearsonr(y_test, pred))\n",
    "\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "print(\"QWK: \", cohen_kappa_score(y_test,pred,weights=\"quadratic\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4223385689354276\n",
      "ِAdjecent Accuracy: 0.8621291448516579\n",
      "pearson correaltion 1  (0.22975158069575605, 2.6635531439179146e-08)\n",
      "pearson correaltion 2 (0.22975158069575605, 2.6635531439179146e-08)\n",
      "[[  0   0   6   2   0   0   0]\n",
      " [  0   0  26  39   0   0   0]\n",
      " [  0   0  51  98   0   0   0]\n",
      " [  0   0  44 191   0   0   0]\n",
      " [  0   0  14  84   0   0   0]\n",
      " [  0   0   3  13   0   0   0]\n",
      " [  0   0   0   2   0   0   0]]\n",
      "QWK:  0.16373445248796503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\AppData\\Local\\Temp\\ipykernel_17008\\2844418081.py:8: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  print(\"pearson correaltion 2\", stats.pearsonr(y_test, pred))\n"
     ]
    }
   ],
   "source": [
    "classifier = DecisionTreeClassifier(max_depth=1) #SVC(kernel=\"linear\", C=0.025) #LogisticRegression()  #DecisionTreeClassifier(max_depth=4)\n",
    "classifier.fit(X_train, y_train)\n",
    "score1 = classifier.score(X_test, y_test)\n",
    "pred = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", score1)\n",
    "print(\"ِAdjecent Accuracy:\",custom_adjacent_accuracy_score(y_test,pred))\n",
    "print(\"pearson correaltion 1 \",scipy.stats.pearsonr(y_test, pred))\n",
    "print(\"pearson correaltion 2\", stats.pearsonr(y_test, pred))\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "print(\"QWK: \", cohen_kappa_score(y_test,pred,weights=\"quadratic\"))\n",
    "\n",
    "\n",
    "# type(essays_train[0])\n",
    "# df_essaysNames = pd.read_excel(\"essaysToFileName.xlsx\")\n",
    "# df_goldStandarsdClusters = pd.read_excel(\"FinalResults.xlsx\")\n",
    "# # essayName = df_essaysNames.query(f\"essay == {essays_train[0]}\")['fileName'].iloc[0]\n",
    "#\n",
    "# res = df_essaysNames[df_essaysNames['essay'].str.contains(essays_train[0])].iloc[0]['fileName']\n",
    "#\n",
    "# print(res)\n",
    "#\n",
    "# listEssayClusers  = df_goldStandarsdClusters[df_goldStandarsdClusters['fileName'].str.contains(res)]['FinalCluster']\n",
    "#\n",
    "# for inn in listEssayClusers:\n",
    "#     print(inn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### tfIdf vectorizer with only n-gram baseline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1716, 12109)\n",
      "(573, 12109)\n",
      "Accuracy: 0.40663176265270506\n",
      "ِAdjecent Accuracy: 0.8481675392670157\n",
      "[[  0   0   4   4   0   0   0]\n",
      " [  0   0  20  43   2   0   0]\n",
      " [  0   0  40 103   6   0   0]\n",
      " [  0   0  39 187   9   0   0]\n",
      " [  0   0  11  81   6   0   0]\n",
      " [  0   0   0  15   1   0   0]\n",
      " [  0   0   0   1   1   0   0]]\n",
      "pearson correaltion 1  (0.2024922409462375, 1.022411070788281e-06)\n",
      "pearson correaltion 2 (0.2024922409462375, 1.022411070788281e-06)\n",
      "weighted kappa:  0.14987146409625973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed\\AppData\\Local\\Temp\\ipykernel_17008\\1195003522.py:32: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  print(\"pearson correaltion 2\", stats.pearsonr(y_test, pred))\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import stats\n",
    "import scipy\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "x\n",
    "essays_train, essays_test, y_train, y_test = train_test_split(essays, y, test_size=0.25, random_state=1000)\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "vectorizer.fit_transform(essays_train)\n",
    "X_train = vectorizer.transform(essays_train)\n",
    "X_test  = vectorizer.transform(essays_test)\n",
    "\n",
    "# X_train = normalize (X_train)\n",
    "# X_test = normalize (X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "classifier = SVC(kernel=\"linear\") ##LogisticRegression() #SVC(kernel=\"linear\", C=0.025) #\n",
    "classifier.fit(X_train, y_train)\n",
    "score1 = classifier.score(X_test, y_test)\n",
    "pred = classifier.predict(X_test)\n",
    "\n",
    "precision, recall, fscore, support = score(y_test, pred)\n",
    "\n",
    "print(\"Accuracy:\", score1)\n",
    "print(\"ِAdjecent Accuracy:\",custom_adjacent_accuracy_score(y_test,pred))\n",
    "# print(\"recall:\", recall, \"fscore:\", fscore, \"support:\", support, \"precision:\", precision)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "print(\"pearson correaltion 1 \",scipy.stats.pearsonr(y_test, pred))\n",
    "print(\"pearson correaltion 2\", stats.pearsonr(y_test, pred))\n",
    "#print(np.corrcoef(\"the value for person correaltaion is: \",np.array(y_test).any(), np.array(pred).any()))\n",
    "print(\"weighted kappa: \", cohen_kappa_score(y_test,pred,weights=\"quadratic\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### tfIdf vectorizer with Clusters as Features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "vectorizer.fit_transform(essays)\n",
    "X_embeddings = vectorizer.transform(essays)\n",
    "\n",
    "df_goldStandarsdClusters = pd.read_excel(\"FinalResults.xlsx\")\n",
    "\n",
    "clusterFeatureVectormulti = []\n",
    "finalX_Train = []\n",
    "\n",
    "for i in range (len(essays)):\n",
    "\n",
    "    clusterVectorFeatureSingle =  [0 for col in range(19)]\n",
    "\n",
    "    listEssayClusers = df_goldStandarsdClusters[df_goldStandarsdClusters['fileName'].str.contains(listRightNames[i])]['cluster']\n",
    "\n",
    "    if (len(listEssayClusers) ==0):\n",
    "        print(\"error\")\n",
    "\n",
    "    for cluster in listEssayClusers:\n",
    "        clusterVectorFeatureSingle[cluster] = clusterVectorFeatureSingle[cluster] + 1\n",
    "\n",
    "    clusterFeatureVectormulti.append(clusterVectorFeatureSingle)\n",
    "\n",
    "Xa = X_embeddings[0]\n",
    "Xb = sparse.csr_matrix(clusterFeatureVectormulti[0])\n",
    "diff_n_rows = Xa.shape[0] - Xb.shape[0]\n",
    "Xb_new = vstack((Xb, csr_matrix((diff_n_rows, Xb.shape[1]))))\n",
    "X_final = hstack((Xa, Xb_new))\n",
    "\n",
    "for i in range(1,len(essays)):\n",
    "    Xa = X_embeddings[i]\n",
    "    Xb = sparse.csr_matrix(clusterFeatureVectormulti[i])\n",
    "    diff_n_rows = Xa.shape[0] - Xb.shape[0]\n",
    "    Xb_new = vstack((Xb, csr_matrix((diff_n_rows, Xb.shape[1]))))\n",
    "    X_semiFinal = hstack((Xa, Xb_new))\n",
    "    X_final = vstack((X_final, X_semiFinal))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.25, random_state=1000)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "classifier = LogisticRegression() #LogisticRegression()  #DecisionTreeClassifier(max_depth=4)\n",
    "classifier.fit(X_train, y_train)\n",
    "score1 = classifier.score(X_test, y_test)\n",
    "pred = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", score1)\n",
    "\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "print(cohen_kappa_score(y_test,pred,weights=\"quadratic\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# listFinal = []\n",
    "# Xa = X_embeddings[0]\n",
    "# Xb = sparse.csr_matrix(clusterFeatureVectormulti[0])\n",
    "#\n",
    "#\n",
    "# diff_n_rows = Xa.shape[0] - Xb.shape[0]\n",
    "#\n",
    "# Xb_new = vstack((Xb, csr_matrix((diff_n_rows, Xb.shape[1]))))\n",
    "# #where diff_n_rows is the difference of the number of rows between Xa and Xb\n",
    "#\n",
    "# X_final = hstack((Xa, Xb_new))\n",
    "#\n",
    "#\n",
    "# Xa = X_embeddings[1]\n",
    "# Xb = sparse.csr_matrix(clusterFeatureVectormulti[1])\n",
    "#\n",
    "#\n",
    "# diff_n_rows = Xa.shape[0] - Xb.shape[0]\n",
    "#\n",
    "# Xb_new = vstack((Xb, csr_matrix((diff_n_rows, Xb.shape[1]))))\n",
    "# #where diff_n_rows is the difference of the number of rows between Xa and Xb\n",
    "#\n",
    "# X_final1 = hstack((Xa, Xb_new))\n",
    "\n",
    "\n",
    "# t = sparse.csr_matrix(clusterFeatureVectormulti[0])\n",
    "# t\n",
    "# X_embeddings[0]\n",
    "\n",
    "# X_final\n",
    "# X_final = vstack((X_final, X_final1))\n",
    "\n",
    "\n",
    "# an_array = np.array([1, 2])\n",
    "# another_array = np.array([3, 4])\n",
    "# stacked_array = np.vstack((an_array, another_array))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# text= ['hello my name is ahmed and I am a data scientist','data science is very important field','data science is not that hard']\n",
    "# text1 = ['you are watching unfold data science']\n",
    "# vectorizer = CountVectorizer()\n",
    "#\n",
    "# v = vectorizer.fit(text)\n",
    "# xv = vectorizer.fit_transform(text)\n",
    "# print(xv.toarray())\n",
    "# print(xv.shape)\n",
    "# vectorizer.get_feature_names_out()\n",
    "# cv = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "# xv = cv.fit_transform(text)\n",
    "# cv.get_feature_names_out()\n",
    "# print(xv.toarray())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import normalize\n",
    "#\n",
    "# norm = normalize (X_train_WithClusers)\n",
    "# norm\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train_WithClusers)\n",
    "# X_TrainedScaled = scaler.fit_transform(X_train_WithClusers)\n",
    "#X_TrainedScaled[0]# from sklearn.preprocessing import normalize\n",
    "#\n",
    "# norm = normalize (X_train_WithClusers)\n",
    "# norm\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train_WithClusers)\n",
    "# X_TrainedScaled = scaler.fit_transform(X_train_WithClusers)\n",
    "#X_TrainedScaled[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Quadratically weighted Kabba - Ready functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# The following 3 functions have been taken from Ben Hamner's github repository\n",
    "# https://github.com/benhamner/Metrics\n",
    "def Cmatrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = y\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = Cmatrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return (1.0 - numerator / denominator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### QWK between the first and the second rater"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "0.3977300242869025"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1QWK = df['Code1_IN_AR_Gesamteindruck_Argumentation_3_4_0']\n",
    "y2QWK = df['Code2_IN_AR_Gesamteindruck_Argumentation_3_4_0']\n",
    "\n",
    "# cohen_kappa_score(y_test,pred,weights=\"quadratic\")\n",
    "\n",
    "cohen_kappa_score(y1QWK,y2QWK,weights=\"quadratic\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fuctions to append the clusters to the feature vectors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "def readTextFromFileName(listNames):\n",
    "    returnList= []\n",
    "    for i in range (len(listNames)):\n",
    "        my_file = Path(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\ALLEssaysAllPrompts\", listNames[i])\n",
    "        if my_file.is_file():\n",
    "            text = my_file.read_text(encoding='utf-8-sig')\n",
    "            returnList.append(text)\n",
    "    return returnList\n",
    "\n",
    "def AppendClustersToTfIDf(essaysNamesList , tfIDFVectorsArray, vectorizer):\n",
    "\n",
    "    df_goldStandarsdClusters = pd.read_excel(\"FinalResults.xlsx\")\n",
    "    clusterFeatureVectormulti = []\n",
    "\n",
    "    for i in range (len(essaysNamesList)):\n",
    "        clusterVectorFeatureSingle =  [0 for col in range(19)]\n",
    "        listEssayClusers = df_goldStandarsdClusters[df_goldStandarsdClusters['fileName'].str.contains(essaysNamesList[i])]['cluster']\n",
    "\n",
    "        if (len(listEssayClusers) ==0):\n",
    "            print(\"error\")\n",
    "\n",
    "        for cluster in listEssayClusers:\n",
    "            clusterVectorFeatureSingle[cluster] = clusterVectorFeatureSingle[cluster] + 1\n",
    "\n",
    "        clusterFeatureVectormulti.append(clusterVectorFeatureSingle)\n",
    "\n",
    "    if (vectorizer == \"tfidf\"):\n",
    "        print(\"tfIDf\")\n",
    "        ### append logic ############\n",
    "        Xa = tfIDFVectorsArray[0]\n",
    "        Xb = sparse.csr_matrix(clusterFeatureVectormulti[0])\n",
    "        diff_n_rows = Xa.shape[0] - Xb.shape[0]\n",
    "        Xb_new = vstack((Xb, csr_matrix((diff_n_rows, Xb.shape[1]))))\n",
    "        X_final = hstack((Xa, Xb_new))\n",
    "\n",
    "        for i in range(1,len(essaysNamesList)):\n",
    "            Xa = tfIDFVectorsArray[i]\n",
    "            Xb = sparse.csr_matrix(clusterFeatureVectormulti[i])\n",
    "            diff_n_rows = Xa.shape[0] - Xb.shape[0]\n",
    "            Xb_new = vstack((Xb, csr_matrix((diff_n_rows, Xb.shape[1]))))\n",
    "            X_semiFinal = hstack((Xa, Xb_new))\n",
    "            X_final = vstack((X_final, X_semiFinal))\n",
    "        return X_final\n",
    "    else:\n",
    "        print(\"sbert\")\n",
    "        finalX_Train = []\n",
    "        for i in range(len(tfIDFVectorsArray)):\n",
    "            arr = np.append(tfIDFVectorsArray[i],clusterFeatureVectormulti[i])\n",
    "            finalX_Train.append(arr)\n",
    "        npa = np.asarray(finalX_Train, dtype=np.ndarray)\n",
    "        return npa\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TFIDF with features Clusters - Increasing the feature vector instead of binary values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1716, 12109)\n",
      "(573, 12109)\n",
      "tfIDf\n",
      "tfIDf\n",
      "(1716, 12128)\n",
      "(573, 12128)\n",
      "Accuracy: 0.37521815008726006\n",
      "ِAdjecent Accuracy: 0.8516579406631762\n",
      "pearson correaltion 1  (0.26751836587620426, 7.579791244585139e-11)\n",
      "pearson correaltion 2 (0.26751836587620426, 7.579791244585139e-11)\n",
      "[[  1   0   4   3   0   0   0]\n",
      " [  1   3  26  32   3   0   0]\n",
      " [  1   5  49  84  10   0   0]\n",
      " [  0   3  50 151  31   0   0]\n",
      " [  0   1  12  75  10   0   0]\n",
      " [  0   0   0  14   1   1   0]\n",
      " [  0   0   0   2   0   0   0]]\n",
      "0.2417616892911011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Ahmed\\AppData\\Local\\Temp\\ipykernel_17008\\4202545438.py:28: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  print(\"pearson correaltion 2\", stats.pearsonr(y_test, pred))\n"
     ]
    }
   ],
   "source": [
    "essaysNames_train, essaysNames_test, y_train, y_test = train_test_split(listRightNames, y, test_size=0.25, random_state=1000)\n",
    "\n",
    "essays_train=readTextFromFileName(essaysNames_train)\n",
    "essays_test =readTextFromFileName(essaysNames_test)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "vectorizer.fit_transform(essays_train)\n",
    "X_train = vectorizer.transform(essays_train)\n",
    "X_test  = vectorizer.transform(essays_test)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "X_train_WithClusers = AppendClustersToTfIDf(essaysNames_train,X_train,\"tfidf\")\n",
    "X_test_WithClusters = AppendClustersToTfIDf(essaysNames_test,X_test,\"tfidf\")\n",
    "print(X_train_WithClusers.shape)\n",
    "print(X_test_WithClusters.shape)\n",
    "\n",
    "### classification logic #######################\n",
    "\n",
    "classifier = LogisticRegression() #LogisticRegression()  #DecisionTreeClassifier(max_depth=4)\n",
    "classifier.fit(X_train_WithClusers, y_train)\n",
    "score1 = classifier.score(X_test_WithClusters, y_test)\n",
    "pred = classifier.predict(X_test_WithClusters)\n",
    "print(\"Accuracy:\", score1)\n",
    "print(\"ِAdjecent Accuracy:\",custom_adjacent_accuracy_score(y_test,pred))\n",
    "print(\"pearson correaltion 1 \",scipy.stats.pearsonr(y_test, pred))\n",
    "print(\"pearson correaltion 2\", stats.pearsonr(y_test, pred))\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "print(cohen_kappa_score(y_test,pred,weights=\"quadratic\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3856893542757417\n",
      "ِAdjecent Accuracy: 0.8656195462478184\n",
      "pearson correaltion 1  (0.2748878145129785, 2.1546903267178903e-11)\n",
      "pearson correaltion 2 (0.2748878145129785, 2.1546903267178903e-11)\n",
      "[[  1   0   5   2   0   0   0]\n",
      " [  0   0  29  33   3   0   0]\n",
      " [  0   3  45  93   8   0   0]\n",
      " [  0   0  47 166  22   0   0]\n",
      " [  0   0   9  80   9   0   0]\n",
      " [  0   0   0  15   1   0   0]\n",
      " [  0   0   0   2   0   0   0]]\n",
      "0.2285871132740509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\AppData\\Local\\Temp\\ipykernel_17008\\593679167.py:9: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  print(\"pearson correaltion 2\", stats.pearsonr(y_test, pred))\n"
     ]
    }
   ],
   "source": [
    "classifier = SVC(kernel=\"linear\") #LogisticRegression()  #DecisionTreeClassifier(max_depth=4)\n",
    "classifier.fit(X_train_WithClusers, y_train)\n",
    "score1 = classifier.score(X_test_WithClusters, y_test)\n",
    "pred = classifier.predict(X_test_WithClusters)\n",
    "print(\"Accuracy:\", score1)\n",
    "print(\"ِAdjecent Accuracy:\",custom_adjacent_accuracy_score(y_test,pred))\n",
    "\n",
    "print(\"pearson correaltion 1 \",scipy.stats.pearsonr(y_test, pred))\n",
    "print(\"pearson correaltion 2\", stats.pearsonr(y_test, pred))\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "print(cohen_kappa_score(y_test,pred,weights=\"quadratic\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sbert with clusters as features with normalized feature vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/54 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "abda481763234825be2311651c7a1abb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/18 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9dd4200353ca4e7383d4ff23680b725c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1716, 768)\n",
      "(573, 768)\n",
      "sbert\n",
      "sbert\n",
      "(1716, 787)\n",
      "(573, 787)\n",
      "Accuracy: 0.3403141361256545\n",
      "ِAdjecent Accuracy: 0.787085514834206\n",
      "[[  1   2   4   1   0   0   0]\n",
      " [  2   6  29  19   7   1   1]\n",
      " [  2  24  51  50  16   6   0]\n",
      " [  0  25  55 104  50   1   0]\n",
      " [  1   3  21  35  32   6   0]\n",
      " [  0   0   2  12   1   1   0]\n",
      " [  0   0   0   0   0   2   0]]\n",
      "0.2685057086165018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "essaysNames_train, essaysNames_test, y_train, y_test = train_test_split(listRightNames, y, test_size=0.25, random_state=1000)\n",
    "\n",
    "essays_train=readTextFromFileName(essaysNames_train)\n",
    "essays_test =readTextFromFileName(essaysNames_test)\n",
    "\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device=\"cuda\")\n",
    "X_train = model.encode(essays_train, show_progress_bar =True, device=\"cuda\")\n",
    "X_test  = model.encode(essays_test, show_progress_bar =True, device=\"cuda\")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "X_train_WithClusers = AppendClustersToTfIDf(essaysNames_train,X_train,\"sbert\")\n",
    "X_test_WithClusters = AppendClustersToTfIDf(essaysNames_test,X_test,\"sbert\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_WithClusers)\n",
    "X_TrainedScaled = scaler.fit_transform(X_train_WithClusers)\n",
    "X_TestScaled = scaler.fit_transform(X_test_WithClusters)\n",
    "\n",
    "print(X_TrainedScaled.shape)\n",
    "print(X_TestScaled.shape)\n",
    "\n",
    "### classification logic #######################\n",
    "\n",
    "classifier = LogisticRegression() #LogisticRegression()  #DecisionTreeClassifier(max_depth=4)\n",
    "classifier.fit(X_TrainedScaled, y_train)\n",
    "score1 = classifier.score(X_TestScaled, y_test)\n",
    "pred = classifier.predict(X_TestScaled)\n",
    "print(\"Accuracy:\", score1)\n",
    "print(\"ِAdjecent Accuracy:\",custom_adjacent_accuracy_score(y_test,pred))\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "print(cohen_kappa_score(y_test,pred,weights=\"quadratic\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3019197207678883\n",
      "ِAdjecent Accuracy: 0.7539267015706806\n",
      "pearson correaltion 1  (0.24563573741579806, 2.54447850065474e-09)\n",
      "pearson correaltion 2 (0.24563573741579806, 2.54447850065474e-09)\n",
      "[[ 1  2  2  3  0  0  0]\n",
      " [ 2 13 32 14  3  1  0]\n",
      " [ 2 31 54 41 16  5  0]\n",
      " [ 0 43 76 74 39  2  1]\n",
      " [ 0 13 22 30 29  4  0]\n",
      " [ 0  0  3  9  2  2  0]\n",
      " [ 0  0  0  1  1  0  0]]\n",
      "0.24072711932242263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\AppData\\Local\\Temp\\ipykernel_17008\\2937301455.py:9: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  print(\"pearson correaltion 2\", stats.pearsonr(y_test, pred))\n"
     ]
    }
   ],
   "source": [
    "classifier = SVC(kernel=\"linear\") #LogisticRegression()  #DecisionTreeClassifier(max_depth=4)\n",
    "classifier.fit(X_TrainedScaled, y_train)\n",
    "score1 = classifier.score(X_TestScaled, y_test)\n",
    "pred = classifier.predict(X_TestScaled)\n",
    "print(\"Accuracy:\", score1)\n",
    "print(\"ِAdjecent Accuracy:\",custom_adjacent_accuracy_score(y_test,pred))\n",
    "\n",
    "print(\"pearson correaltion 1 \",scipy.stats.pearsonr(y_test, pred))\n",
    "print(\"pearson correaltion 2\", stats.pearsonr(y_test, pred))\n",
    "\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "print(cohen_kappa_score(y_test,pred,weights=\"quadratic\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import fasttext"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "model = fasttext.train_unsupervised('data/test.txt', model='cbow',minCount=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test = \"This is a exmaple of fastText\"\n",
    "\n",
    "model = fasttext.train_supervised('data/test.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'exmaple', 'of', 'fastText']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(model.words)\n",
    "print(model.labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}