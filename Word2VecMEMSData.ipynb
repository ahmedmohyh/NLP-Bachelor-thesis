{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Importing section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from nltk import tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from nltk.corpus import stopwords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reading the data from the folder and cleaning it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for filename in os.listdir(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\TelevisonMergedT1+T2\"):\n",
    "   with open(os.path.join(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\TelevisonMergedT1+T2\", filename)) as f:\n",
    "       text = f.read()\n",
    "       text = text.replace(\"ï»¿\",\"\")\n",
    "       sents = tokenize.sent_tokenize(text)\n",
    "       for s in sents:\n",
    "           #s = s.lower()\n",
    "           #s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "           sentences.append(s)\n",
    "\n",
    "tokensSentenceslist = []\n",
    "\n",
    "for s  in sentences:\n",
    "    wordsList = gensim.utils.simple_preprocess(s)\n",
    "    filtered_words = [word for word in wordsList if word not in stopwords.words('english')]\n",
    "    tokensSentenceslist.append(filtered_words)\n",
    "\n",
    "\n",
    "##################### Uncomment below section for testing #########################\n",
    "# print(len(sentences))\n",
    "#\n",
    "# for s in sentences:\n",
    "#      print(\"The sentence is : \")\n",
    "#      print(s)\n",
    "#      print(\"-----------------------End of the sentence -------------\")\n",
    "#\n",
    "# print (sentences)\n",
    "\n",
    "\n",
    "# print (len(tokensSentenceslist))\n",
    "# print (tokensSentenceslist)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['advertising', 'television', 'becoming', 'clever']\n"
     ]
    }
   ],
   "source": [
    "print (tokensSentenceslist[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'just', 'test', 'and', 'don', 'know']\n"
     ]
    }
   ],
   "source": [
    "testSentences = gensim.utils.simple_preprocess(\"This is just a test and i don't know\")\n",
    "\n",
    "print (testSentences)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38715\n",
      "['Advertising in Television is becoming more and more clever.', 'Lots of those ads invade our subconsciousness and try to make us buy the product even if we do not actually need it.', \"When it comes to children, lots of them don't have enough money to afford these products.\", \"They don't even understand the concept of money.\", 'Even if they wanted it, they can not get the thing they were told they need.']\n"
     ]
    }
   ],
   "source": [
    "print (len(sentences))\n",
    "print(sentences[0:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generating the Word2Vec Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "outputs": [
    {
     "data": {
      "text/plain": "(1313264, 1737390)"
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = Word2Vec(tokensSentenceslist, min_count=1)\n",
    "\n",
    "#model = Word2Vec(tokensSentenceslist, vector_size=50, min_count=1, sg=1)\n",
    "#model = Word2Vec(sentences=tokensSentenceslist, vector_size=100, workers=1, seed=42)\n",
    "\n",
    "model = Word2Vec(window=10, min_count=2,workers=6,vector_size=50,seed=42,sg=0)\n",
    "model.build_vocab(tokensSentenceslist, progress_per=1000)\n",
    "model.train(tokensSentenceslist, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "\n",
    "##################### Uncomment below section for testing #########################\n",
    "\n",
    "\n",
    "# print(list(model.wv.index_to_key))\n",
    "# print(len(list(model.wv.index_to_key)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [
    {
     "data": {
      "text/plain": "38715"
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count\n",
    "#model.epochs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "outputs": [
    {
     "data": {
      "text/plain": "[('point', 0.9456915855407715),\n ('reason', 0.9341050386428833),\n ('side', 0.9020331501960754),\n ('aspect', 0.8889020681381226),\n ('agree', 0.8727644085884094),\n ('support', 0.868994414806366),\n ('reasons', 0.8553298711776733),\n ('aspects', 0.8501182794570923),\n ('disagree', 0.8476272821426392),\n ('personally', 0.8415126204490662)]"
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.wv.most_similar(\"television\")\n",
    "model.wv.most_similar(\"argument\")\n",
    "#model.wv.similarity(\"tv\",\"television\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vectorizing each sentence using the avg of the Word embidings of each word"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "outputs": [],
   "source": [
    "def vectorize(list_of_docs, model, strategy):\n",
    "    \"\"\"Generate vectors for list of documents using a Word Emx`bedding.\n",
    "\n",
    "    Args:\n",
    "        list_of_docs: List of documents.\n",
    "        model: Gensim Word Embedding.\n",
    "        strategy: Aggregation strategy (\"average\", or \"min-max\".)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the strategy is other than \"average\" or \"min-max\".\n",
    "\n",
    "    Returns:\n",
    "        List of vectors.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    size_output = model.vector_size\n",
    "    embedding_dict = model\n",
    "\n",
    "    if strategy == \"min-max\":\n",
    "        size_output *= 2\n",
    "\n",
    "    if hasattr(model, \"wv\"):\n",
    "        embedding_dict = model.wv\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(size_output)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in embedding_dict:\n",
    "                try:\n",
    "                    vectors.append(embedding_dict[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            if strategy == \"min-max\":\n",
    "                min_vec = vectors.min(axis=0)\n",
    "                max_vec = vectors.max(axis=0)\n",
    "                features.append(np.concatenate((min_vec, max_vec)))\n",
    "            elif strategy == \"average\":\n",
    "                avg_vec = vectors.mean(axis=0)\n",
    "                features.append(avg_vec)\n",
    "            else:\n",
    "                raise ValueError(f\"Aggregation strategy {strategy} does not exist!\")\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Apply the function above"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "outputs": [],
   "source": [
    "vectorized_docs = vectorize(tokensSentenceslist, model=model, strategy=\"average\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.0167569e-01 -7.7913541e-01  9.7707295e-01  8.8773452e-04\n",
      " -3.0638605e-01 -2.1325390e-01 -1.0785919e+00 -2.6945767e-01\n",
      "  2.0254721e-01  2.7150404e-01  6.7686290e-01  9.5995587e-01\n",
      "  8.0758250e-01  1.0841374e+00  9.6509784e-01  8.8220216e-02\n",
      "  8.1530887e-01 -1.7672682e+00 -6.3352096e-01  5.6337196e-01\n",
      " -6.2989330e-01 -2.4646464e-01  5.2156770e-01  2.5482684e-01\n",
      "  5.2680212e-01  2.7302524e-01 -3.9422449e-01 -6.4586878e-01\n",
      "  8.5464883e-01  5.9058088e-01 -2.6026830e-01 -1.0212562e+00\n",
      " -5.1818579e-02 -4.1434370e-02  6.6938829e-01  5.3291500e-01\n",
      " -5.5190966e-02 -3.0560575e+00 -5.5581880e-01  3.9764848e-01\n",
      " -1.5702914e-01 -5.5771077e-01  1.3291311e+00 -9.2295527e-01\n",
      " -5.7646757e-01  4.1336891e-01 -1.7303368e-01  7.0126784e-01\n",
      "  9.2575854e-01 -1.5619569e-01]\n",
      "#######################################################\n",
      "[ 0.13167055 -0.9581087   0.18906346 -0.02515565 -0.16255502 -0.37995827\n",
      "  0.11477079  0.11843309  0.07814962  0.42332467  0.01161281  0.32912475\n",
      "  0.08110611  1.1597311   0.3279072   0.09340206  0.49939045 -0.9927523\n",
      " -0.33739397  0.05179083 -0.7677206  -0.01766879 -0.18057902  0.13510114\n",
      "  0.812325    0.20954396 -0.15844423  0.14122114 -0.53450644  0.23872726\n",
      " -0.04537928 -0.11853724 -0.18760931  0.14190808  0.9345352  -0.39498144\n",
      " -0.27279705 -0.9956446  -0.20677863  0.02775279 -0.33310837  0.17689833\n",
      "  0.45595923 -0.2715257   0.34521025 -0.11242506  0.06749877  0.7201002\n",
      "  0.5238283  -0.6268167 ]\n"
     ]
    }
   ],
   "source": [
    "len(vectorized_docs), len(vectorized_docs[0])\n",
    "print(model.wv[\"argument\"])\n",
    "\n",
    "print(\"#######################################################\")\n",
    "print(vectorized_docs[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Kmeans algorithm with mini batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "outputs": [],
   "source": [
    "def mbkmeans_clusters(X, k, mb=500, print_silhouette_values=False):\n",
    "    \"\"\"Generate clusters.\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches. Defaults to 500.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Applying the Kmeans algorithm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 15\n",
      "Silhouette coefficient: 0.13\n",
      "Inertia:90663.38629044882\n",
      "Silhouette values:\n",
      "    Cluster 8: Size:163 | Avg:1.00 | Min:0.86 | Max: 1.00\n",
      "    Cluster 2: Size:492 | Avg:0.62 | Min:0.17 | Max: 0.66\n",
      "    Cluster 0: Size:1455 | Avg:0.28 | Min:-0.02 | Max: 0.47\n",
      "    Cluster 6: Size:551 | Avg:0.25 | Min:-0.19 | Max: 0.48\n",
      "    Cluster 1: Size:4929 | Avg:0.21 | Min:-0.03 | Max: 0.44\n",
      "    Cluster 11: Size:805 | Avg:0.14 | Min:-0.11 | Max: 0.39\n",
      "    Cluster 7: Size:3569 | Avg:0.14 | Min:-0.05 | Max: 0.35\n",
      "    Cluster 13: Size:3009 | Avg:0.12 | Min:-0.06 | Max: 0.34\n",
      "    Cluster 9: Size:2435 | Avg:0.11 | Min:-0.08 | Max: 0.33\n",
      "    Cluster 4: Size:3555 | Avg:0.10 | Min:-0.06 | Max: 0.30\n",
      "    Cluster 3: Size:5917 | Avg:0.09 | Min:-0.08 | Max: 0.30\n",
      "    Cluster 12: Size:1679 | Avg:0.09 | Min:-0.14 | Max: 0.36\n",
      "    Cluster 10: Size:3001 | Avg:0.08 | Min:-0.11 | Max: 0.32\n",
      "    Cluster 5: Size:5273 | Avg:0.08 | Min:-0.09 | Max: 0.28\n",
      "    Cluster 14: Size:1882 | Avg:0.08 | Min:-0.12 | Max: 0.31\n"
     ]
    }
   ],
   "source": [
    "clustering, cluster_labels = mbkmeans_clusters(X=vectorized_docs, k=15, print_silhouette_values=True)\n",
    "\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"text\": sentences,\n",
    "    \"tokens\": [\" \".join(text) for text in tokensSentenceslist],\n",
    "    \"cluster\": cluster_labels\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate top terms of the cluster"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Top terms per cluster (based on centroids):\")\n",
    "for i in range(10): # number of cluster k should be put here!!\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[i]], topn=10)\n",
    "    #print(clustering.cluster_centers_[i])\n",
    "    for t in most_representative:\n",
    "        tokens_per_cluster += f\"{t[0]} \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 287,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster (based on centroids):\n",
      "Cluster 0: nutshell allowed difficult opinion forbidden allowd childern concluding anyway ages \n",
      "Cluster 1: behave others theire addicted bored freetime entertaint entertain forget healthy \n",
      "Cluster 2: pour water cat catching mcdonald shoes machine die sie hell \n",
      "Cluster 3: difference furthermore canâ worse must arenâ helpful childen donâ handle \n",
      "Cluster 4: advertises necessary firstly advertisings anyway difficult matter adversting complicated however \n",
      "Cluster 5: probably tell someone anything articles automatically feeling stop mad annoy \n",
      "Cluster 6: mean means actually impress course sure exactly ones makes cannot \n",
      "Cluster 7: anyway shouldnâ musst controll letting sometimes whatch put arenâ childen \n",
      "Cluster 8: discuss answer dicuss write following reasons theme sides discussed discussion \n",
      "Cluster 9: cant worse partents theirselves childen cases addition possibility chance partens \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: children(3108) young(2356) advertising(2148) television(1847) directed(1647) \n",
      "Cluster 1: children(629) learn(557) play(547) time(440) outside(433) \n",
      "Cluster 2: sasageyo(1092) die(654) qualitã(325) zeit(325) sie(324) \n",
      "Cluster 3: children(3045) things(1078) parents(907) get(788) young(786) \n",
      "Cluster 4: children(4398) advertising(2718) young(2248) television(2007) think(1158) \n",
      "Cluster 5: parents(2150) want(1793) buy(1532) children(1397) child(1159) \n",
      "Cluster 6: children(925) products(692) money(689) make(571) companies(560) \n",
      "Cluster 7: tv(2194) watch(2138) children(2013) television(1349) watching(885) \n",
      "Cluster 8: statement(331) schreibaufgabe(293) agree(260) following(211) disagree(191) \n",
      "Cluster 9: children(1161) advertising(631) television(593) like(536) would(536) \n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for i in range(10):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_frequent = Counter(\" \".join(df_clusters.query(f\"cluster == {i}\")[\"tokens\"]).split()).most_common(5)\n",
    "    for t in most_frequent:\n",
    "        tokens_per_cluster += f\"{t[0]}({str(t[1])}) \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Retrieve a random sample of documents for a given cluster\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the following text I am going to state my opinion on the statement: Television advertising directed toward young children (aged two to five) should not be allowed.\n",
      "-------------\n",
      "Televison advertising towards kids between the age of two to five are in my opinion both good and bad.\n",
      "-------------\n",
      "Then we come to the next negative point by television advertising toward young children.\n",
      "-------------\n",
      "Should television advertising directed toward young children be allowed or is it just a dumb thing?\n",
      "-------------\n",
      "Firstly, Television toward young children influences them in various ways.\n",
      "-------------\n",
      "I came to the conclusion that television advertising aimed towards young children should be prohibited.\n",
      "-------------\n",
      "The following essay talks about television advertising to young children.\n",
      "-------------\n",
      "There is no definition of advertising for a two year old.\n",
      "-------------\n",
      "Not only the advertising.\n",
      "-------------\n",
      "In this essay, the question about banning television ads directed toward young children will be responded.\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "for i,t in enumerate(df_clusters.query(f\"cluster == {0}\").sample(10).iterrows()):\n",
    "    print(t[1][\"text\"])\n",
    "    print(\"-------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "outputs": [],
   "source": [
    "df_clusters.to_csv('clusteredArgument.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "outputs": [
    {
     "data": {
      "text/plain": "(38715, 3)"
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clusters.shape\n",
    "\n",
    "# df_clusters.tokens[0]\n",
    "# df_clusters.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Most representative clusters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8110\n",
      "The statement \"Television advertising directed toward young children (aged two or five) should not be allowed\" says that television are directing advertisement towards children, what in my opinion is not okay.\n",
      "-------------\n",
      "8335\n",
      "In conclusion I can certainly say that television and internet advertising directed toward young children (aged two to five) should not be allowed.\n",
      "-------------\n",
      "34248\n",
      "The question whether television advertising directed toward young children, from age two to five, should be forbidden by law is controversial.\n",
      "-------------\n",
      "35721\n",
      "This text should be helpful for parents who have young children aged two to five because here we awnser the question if television advertising directed toward young children should be allowed or not!\n",
      "-------------\n",
      "36294\n",
      "Personally, I think that television advertising directed toward young children (aged two to five) should not be allowed, because it has a negative influence on the childhood.\n",
      "-------------\n",
      "18548\n",
      "Today i would like to discuss the statement ,,Television advertising directed toward young children (aged two to five) should not be allowed.''\n",
      "-------------\n",
      "3713\n",
      "The statement\"Television advertising directed toward young children (aged two to five) should not be allowed\", is a dificult theme for the childrens and for the parents.\n",
      "-------------\n",
      "15288\n",
      "The statement \"Television advertising directed toward young children (aged two to five) should not be allowed\" is a very controversy but actual theme.\n",
      "-------------\n",
      "15774\n",
      "This leads me to the statement, I read in an article, that television advertising directed toward young children, aged two to five, should not be allowed.\n",
      "-------------\n",
      "29313\n",
      "Well, there are many reasons why television advertising directed towards children aged two to five should not be allowed.\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "test_cluster = 0\n",
    "most_representative_docs = np.argsort(\n",
    "    np.linalg.norm(vectorized_docs - clustering.cluster_centers_[test_cluster], axis=1)\n",
    ")\n",
    "# print(most_representative_docs[0])\n",
    "for d in most_representative_docs[:10]:\n",
    "    print(d)\n",
    "    print(sentences[d])\n",
    "    print(\"-------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1])"
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#array = clustering.cluster_centers_[0]\n",
    "#print(len(vectorized_docs))\n",
    "#print(array)\n",
    "array = vectorized_docs[120].reshape(1,-1)\n",
    "convertedArray = array.astype(float)\n",
    "# vectorized_docs[i] = sentences[i] it is the same\n",
    "clustering.predict(convertedArray)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.41131788 -0.11716345  0.28971842  0.5577258  -0.5772017   0.9901962\n",
      "   0.96931005 -0.5613676  -0.3226952  -0.4045963   0.18726283  0.44057044\n",
      "  -0.42001647 -0.4549887  -0.8095281   0.07320678 -0.20656103 -0.8354379\n",
      "   0.20628421 -0.6952043  -0.29465434 -0.11770947 -0.03002735  0.10196522\n",
      "   0.37290215 -0.10642844  0.10388153 -0.5029228  -0.48719856  0.58507514\n",
      "  -0.10985485 -0.43433896 -0.43355358 -0.5003478  -0.255086    1.0371667\n",
      "  -0.56815755 -1.0393796  -0.58956033  0.4535848  -0.05363839 -0.7777182\n",
      "   1.1643625  -0.07477699 -0.07372791 -0.9933421  -0.22898763 -0.10050163\n",
      "  -0.13402921 -0.31821993]]\n",
      "[[ 0.41131788 -0.11716345  0.28971842  0.55772579 -0.57720172  0.99019623\n",
      "   0.96931005 -0.56136757 -0.3226952  -0.4045963   0.18726283  0.44057044\n",
      "  -0.42001647 -0.45498869 -0.80952811  0.07320678 -0.20656103 -0.83543789\n",
      "   0.20628421 -0.69520432 -0.29465434 -0.11770947 -0.03002735  0.10196522\n",
      "   0.37290215 -0.10642844  0.10388153 -0.50292277 -0.48719856  0.58507514\n",
      "  -0.10985485 -0.43433896 -0.43355358 -0.50034779 -0.255086    1.03716671\n",
      "  -0.56815755 -1.0393796  -0.58956033  0.45358479 -0.05363839 -0.77771819\n",
      "   1.16436255 -0.07477699 -0.07372791 -0.9933421  -0.22898763 -0.10050163\n",
      "  -0.13402921 -0.31821993]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\AppData\\Local\\Temp\\ipykernel_25384\\3934604238.py:8: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  convertedArray = array.astype(np.float)\n"
     ]
    }
   ],
   "source": [
    "#print(array.reshape(1,-1))\n",
    "\n",
    "print(array)\n",
    "\n",
    "print(convertedArray)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predicting new clusters for testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n"
     ]
    }
   ],
   "source": [
    "## testing\n",
    "def vectorizeSentenceTest(sentences):\n",
    "    tokensSentenceslist = []\n",
    "    for s  in sentences:\n",
    "        wordsList = word_tokenize(s)\n",
    "        tokensSentenceslist.append(wordsList)\n",
    "    return tokensSentenceslist\n",
    "\n",
    "testTokens = vectorizeSentenceTest([\"Watching tv for long hours leads to lazness\"])\n",
    "\n",
    "vectorized_docs_tesing = vectorize(testTokens, model=model, strategy=\"average\")\n",
    "\n",
    "def predictTest(vectorizedDocsTest):\n",
    "    array = vectorizedDocsTest\n",
    "    print(clustering.predict(array))\n",
    "    return\n",
    "#len(vectorized_docs_tesing), len(vectorized_docs_tesing[0])\n",
    "#print(vectorized_docs_tesing)\n",
    "\n",
    "predictTest(vectorized_docs_tesing)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list\n",
      "(38715,)\n"
     ]
    }
   ],
   "source": [
    "from Cython import typeof\n",
    "\n",
    "print(typeof(vectorized_docs))\n",
    "\n",
    "# print(vectorized_docs[0])\n",
    "# print(vectorized_docs[0].shape)\n",
    "# print (\"############################################\")\n",
    "#\n",
    "# print(clustering.cluster_centers_[0])\n",
    "# print(clustering.cluster_centers_[0].shape)\n",
    "print (most_representative_docs.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "itemindex = np.where(vectorized_docs == clustering.cluster_centers_[0])\n",
    "\n",
    "print(itemindex)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "outputs": [],
   "source": [
    "testDocs = np.sort(\n",
    "    np.linalg.norm(vectorized_docs - clustering.cluster_centers_[0], axis=1)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02014501 -0.8525229  -0.04869652 -0.13764937 -0.15015027  0.050198\n",
      " -0.16530403  0.2516511  -0.17082413  0.35278627  0.10296659  0.5194729\n",
      "  0.14524572  1.0872988   0.4511473   0.17673676  0.23570392 -1.1361816\n",
      " -0.38236055 -0.02064579 -0.5204541   0.09028093 -0.1568032   0.39367923\n",
      "  0.9687556   0.00753525 -0.11855845  0.01320843 -0.4333722   0.24393816\n",
      " -0.30497146 -0.32613945 -0.07790004  0.1395214   0.75188285 -0.22694431\n",
      " -0.32751098 -1.3844128  -0.04305803  0.2324039  -0.29520527  0.21222845\n",
      "  0.75226027 -0.36937404  0.07873303 -0.20217176  0.27925226  1.0212195\n",
      "  0.86630887 -0.4531421 ]\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_docs[16035])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing wether the centroid are sentences or not"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "itemindex = np.where(vectorized_docs == clustering.cluster_centers_[9])\n",
    "\n",
    "print(itemindex[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}