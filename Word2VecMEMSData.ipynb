{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Importing section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from nltk import tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from nltk.corpus import stopwords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reading the data from the folder and cleaning it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for filename in os.listdir(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\TelevisonMergedT1+T2\"):\n",
    "   with open(os.path.join(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\TelevisonMergedT1+T2\", filename)) as f:\n",
    "       text = f.read()\n",
    "       text = text.replace(\"ï»¿\",\"\")\n",
    "       sents = tokenize.sent_tokenize(text)\n",
    "       for s in sents:\n",
    "           #s = s.lower()\n",
    "           #s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "           sentences.append(s)\n",
    "\n",
    "tokensSentenceslist = []\n",
    "\n",
    "for s  in sentences:\n",
    "    wordsList = gensim.utils.simple_preprocess(s)\n",
    "    filtered_words = [word for word in wordsList if word not in stopwords.words('english')]\n",
    "    tokensSentenceslist.append(filtered_words)\n",
    "\n",
    "\n",
    "##################### Uncomment below section for testing #########################\n",
    "# print(len(sentences))\n",
    "#\n",
    "# for s in sentences:\n",
    "#      print(\"The sentence is : \")\n",
    "#      print(s)\n",
    "#      print(\"-----------------------End of the sentence -------------\")\n",
    "#\n",
    "# print (sentences)\n",
    "\n",
    "\n",
    "# print (len(tokensSentenceslist))\n",
    "# print (tokensSentenceslist)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['advertising', 'television', 'becoming', 'clever']\n"
     ]
    }
   ],
   "source": [
    "print (tokensSentenceslist[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'just', 'test', 'and', 'don', 'know']\n"
     ]
    }
   ],
   "source": [
    "testSentences = gensim.utils.simple_preprocess(\"This is just a test and i don't know\")\n",
    "\n",
    "print (testSentences)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38715\n",
      "['Advertising in Television is becoming more and more clever.', 'Lots of those ads invade our subconsciousness and try to make us buy the product even if we do not actually need it.', \"When it comes to children, lots of them don't have enough money to afford these products.\", \"They don't even understand the concept of money.\", 'Even if they wanted it, they can not get the thing they were told they need.']\n"
     ]
    }
   ],
   "source": [
    "print (len(sentences))\n",
    "print(sentences[0:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generating the Word2Vec Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(1313549, 1737390)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = Word2Vec(tokensSentenceslist, min_count=1)\n",
    "\n",
    "#model = Word2Vec(tokensSentenceslist, vector_size=50, min_count=1, sg=1)\n",
    "#model = Word2Vec(sentences=tokensSentenceslist, vector_size=100, workers=1, seed=42)\n",
    "\n",
    "model = Word2Vec(window=10, min_count=2,workers=6,vector_size=100,seed=42,sg=0)\n",
    "model.build_vocab(tokensSentenceslist, progress_per=1000)\n",
    "model.train(tokensSentenceslist, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "\n",
    "##################### Uncomment below section for testing #########################\n",
    "\n",
    "\n",
    "# print(list(model.wv.index_to_key))\n",
    "# print(len(list(model.wv.index_to_key)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "38715"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count\n",
    "#model.epochs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[('reason', 0.9564138650894165),\n ('point', 0.947143018245697),\n ('side', 0.9210261106491089),\n ('support', 0.9066071510314941),\n ('points', 0.8929862380027771),\n ('agree', 0.8925236463546753),\n ('aspect', 0.88764488697052),\n ('positive', 0.8861747980117798),\n ('reasons', 0.8837171196937561),\n ('disagree', 0.8779966235160828)]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.wv.most_similar(\"television\")\n",
    "model.wv.most_similar(\"argument\")\n",
    "#model.wv.similarity(\"tv\",\"television\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vectorizing each sentence using the avg of the Word embidings of each word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def vectorize(list_of_docs, model, strategy):\n",
    "    \"\"\"Generate vectors for list of documents using a Word Emx`bedding.\n",
    "\n",
    "    Args:\n",
    "        list_of_docs: List of documents.\n",
    "        model: Gensim Word Embedding.\n",
    "        strategy: Aggregation strategy (\"average\", or \"min-max\".)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the strategy is other than \"average\" or \"min-max\".\n",
    "\n",
    "    Returns:\n",
    "        List of vectors.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    size_output = model.vector_size\n",
    "    embedding_dict = model\n",
    "\n",
    "    if strategy == \"min-max\":\n",
    "        size_output *= 2\n",
    "\n",
    "    if hasattr(model, \"wv\"):\n",
    "        embedding_dict = model.wv\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(size_output)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in embedding_dict:\n",
    "                try:\n",
    "                    vectors.append(embedding_dict[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            if strategy == \"min-max\":\n",
    "                min_vec = vectors.min(axis=0)\n",
    "                max_vec = vectors.max(axis=0)\n",
    "                features.append(np.concatenate((min_vec, max_vec)))\n",
    "            elif strategy == \"average\":\n",
    "                avg_vec = vectors.mean(axis=0)\n",
    "                features.append(avg_vec)\n",
    "            else:\n",
    "                raise ValueError(f\"Aggregation strategy {strategy} does not exist!\")\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Apply the function above"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "vectorized_docs = vectorize(tokensSentenceslist, model=model, strategy=\"average\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38715 100\n",
      "[ 0.11528822  0.18933415 -0.08412005  0.10589357 -0.4265991   1.0531337\n",
      " -0.13715024  1.298568   -0.8779157  -0.1582561  -0.0780249   0.9799034\n",
      "  0.44961306  0.5217004  -0.37189397 -0.7606606  -0.12328722 -1.425695\n",
      " -0.83165705  0.6247762  -0.95108575 -0.16939847  0.6635875   0.8707157\n",
      "  0.59455246  0.29358917 -0.42626062  0.38870233 -0.44945085 -0.76935554\n",
      " -0.42892438  0.149744   -0.04381364 -0.356411    0.37657246 -0.4028334\n",
      "  0.43931213 -0.53439194 -0.19522092  0.3774061   0.1344047  -0.5865974\n",
      "  0.07677748  0.10171629 -0.21673799  0.46849895  0.36986643  0.5009073\n",
      "  0.76379424 -0.7040103   0.36664996 -0.1903421  -0.61849546  0.01906019\n",
      "  0.5785666   0.04054277 -0.06649713  0.02076649 -0.6030253   0.1796957\n",
      " -0.4925605   0.7685024  -1.3589346   0.18270253 -0.3059741  -0.17928098\n",
      "  0.43794832 -0.28152093 -1.6943305  -0.2687159   0.7706033  -0.56518394\n",
      " -0.16859932 -0.23504497  0.4105878  -0.04653804 -0.9585702  -0.91110015\n",
      "  0.09044305 -0.11684474 -0.07303404 -0.5460595  -0.24975692  0.56739587\n",
      " -0.16196078  0.1683048  -0.915627   -0.5133682   0.12869072 -0.24628666\n",
      "  0.33810633  0.78459334 -0.08717226  0.3919986  -0.2833708  -0.35444993\n",
      "  0.4050759  -0.05124554 -0.406308   -0.2978393 ]\n",
      "#######################################################\n",
      "[ 0.12337929  0.16843857  0.0658899  -0.28559533 -0.07402971  0.385294\n",
      "  0.27202722  0.606674   -0.2515142  -0.43737975 -0.36877352  0.59321105\n",
      "  0.17567089  0.22579153 -0.14748251 -0.37957245  0.01682236 -0.6520853\n",
      " -0.6440546   0.31553936 -0.1656778  -0.23007561 -0.06271269  0.43359613\n",
      "  0.2172175   0.17629644 -0.20035018  0.43459186 -0.3057828   0.0268719\n",
      " -0.15756315 -0.07723028 -0.37749648 -0.0885497   0.02426545  0.00999929\n",
      "  0.29198414 -0.6063522  -0.6432613  -0.05996403  0.05524503 -0.40862978\n",
      "  0.01886777 -0.36034194  0.06876882  0.16067763  0.1802302   0.08137114\n",
      "  0.46828768 -0.43694612  0.25001448 -0.64632493 -0.12705377 -0.34187868\n",
      "  0.12920779 -0.15870151 -0.04334836  0.20909207 -0.06623267  0.24780934\n",
      " -0.04783104 -0.0263225  -0.6827534   0.20918392  0.22675154  0.11793347\n",
      "  0.2925877  -0.3982932  -0.763431   -0.14450136 -0.19299448 -0.25127774\n",
      " -0.13276906  0.0079531   0.29367894 -0.39281207 -0.27650177 -0.0404732\n",
      " -0.17364217 -0.24822329 -0.01426954 -0.28638917 -0.47229612 -0.04590126\n",
      " -0.2512451   0.49466    -0.49726513 -0.54725176 -0.19786467  0.1179613\n",
      " -0.28508893  0.31812155  0.13628575 -0.18266562  0.4828959   0.07577346\n",
      " -0.25983724  0.06075483 -0.4882914   0.0013838 ]\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorized_docs), len(vectorized_docs[0]))\n",
    "print(model.wv[\"argument\"])\n",
    "\n",
    "print(\"#######################################################\")\n",
    "print(vectorized_docs[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Kmeans algorithm with mini batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def mbkmeans_clusters(X, k, mb=500, print_silhouette_values=False):\n",
    "    \"\"\"Generate clusters.\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches. Defaults to 500.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Applying the Kmeans algorithm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 10\n",
      "Silhouette coefficient: 0.14\n",
      "Inertia:103203.68269933804\n",
      "Silhouette values:\n",
      "    Cluster 7: Size:655 | Avg:0.58 | Min:0.21 | Max: 0.67\n",
      "    Cluster 2: Size:313 | Avg:0.45 | Min:-0.02 | Max: 0.50\n",
      "    Cluster 3: Size:2442 | Avg:0.25 | Min:-0.04 | Max: 0.46\n",
      "    Cluster 4: Size:6745 | Avg:0.16 | Min:-0.05 | Max: 0.37\n",
      "    Cluster 5: Size:647 | Avg:0.14 | Min:-0.09 | Max: 0.42\n",
      "    Cluster 0: Size:5430 | Avg:0.13 | Min:-0.13 | Max: 0.38\n",
      "    Cluster 9: Size:5366 | Avg:0.12 | Min:-0.10 | Max: 0.36\n",
      "    Cluster 8: Size:8451 | Avg:0.11 | Min:-0.07 | Max: 0.30\n",
      "    Cluster 1: Size:6422 | Avg:0.09 | Min:-0.10 | Max: 0.30\n",
      "    Cluster 6: Size:2244 | Avg:0.07 | Min:-0.11 | Max: 0.33\n"
     ]
    }
   ],
   "source": [
    "clustering, cluster_labels = mbkmeans_clusters(X=vectorized_docs, k=10, print_silhouette_values=True)\n",
    "\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"text\": sentences,\n",
    "    \"tokens\": [\" \".join(text) for text in tokensSentenceslist],\n",
    "    \"cluster\": cluster_labels\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate top terms of the cluster"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Top terms per cluster (based on centroids):\")\n",
    "for i in range(10): # number of cluster k should be put here!!\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[i]], topn=10)\n",
    "    #print(clustering.cluster_centers_[i])\n",
    "    for t in most_representative:\n",
    "        tokens_per_cluster += f\"{t[0]} \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster (based on centroids):\n",
      "Cluster 0: sometimes stupid arenâ anyway electronics must whatch wathing theire childen \n",
      "Cluster 1: childern anyway necessary adversting permit advertisings advertises think firstly difficult \n",
      "Cluster 2: outweigh controverse discussion schreibaufgabe iâ cons answers controversy write answer \n",
      "Cluster 3: allowed forbidden opinion adversting difficult personally permit childern question ages \n",
      "Cluster 4: sort secondly addition absolutly advertsing overwhelmed doubt smart danger risks \n",
      "Cluster 5: reasons disagree personally sides sum points conclusion difficult arguments conclude \n",
      "Cluster 6: bored else fun anymore healthy forget together seeing encourage love \n",
      "Cluster 7: shoes stuffed bar cream ils drink truck fantasies wheels wars \n",
      "Cluster 8: furthermore useful difference case worse sure canâ belive thats cannot \n",
      "Cluster 9: probably decision stop anything course mean cannot mad actually makes \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: tv(2638) children(2377) watch(2097) television(1614) watching(1083) \n",
      "Cluster 1: children(5062) young(2767) advertising(2666) television(2189) think(1163) \n",
      "Cluster 2: schreibaufgabe(293) essay(161) following(156) statement(151) welches(149) \n",
      "Cluster 3: children(2164) young(1744) advertising(1682) television(1417) directed(1390) \n",
      "Cluster 4: children(747) advertising(662) television(560) would(429) like(412) \n",
      "Cluster 5: statement(327) agree(171) arguments(118) following(90) opinion(77) \n",
      "Cluster 6: play(599) children(417) new(393) like(365) outside(345) \n",
      "Cluster 7: sasageyo(1092) die(654) qualitã(325) zeit(325) sie(324) \n",
      "Cluster 8: children(4031) advertising(1443) young(1126) television(1077) things(1039) \n",
      "Cluster 9: parents(2376) want(1897) buy(1872) children(1832) child(1018) \n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for i in range(10):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_frequent = Counter(\" \".join(df_clusters.query(f\"cluster == {i}\")[\"tokens\"]).split()).most_common(5)\n",
    "    for t in most_frequent:\n",
    "        tokens_per_cluster += f\"{t[0]}({str(t[1])}) \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Retrieve a random sample of documents for a given cluster\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And if they watch there shouldn't be advertising in between the series they are watching.\n",
      "-------------\n",
      "If the children look TV, they get attracted by this advertisement.\n",
      "-------------\n",
      "You can barely find 7 years old kids on instagram.\n",
      "-------------\n",
      "Moreover there are much more effective, healthy and communnicative ways for children to learn a new language and imrpove their speaking skills than watching television.\n",
      "-------------\n",
      "For example the german tv channel 'KIKA'.\n",
      "-------------\n",
      "A TV can not attack you or do anything by its own.\n",
      "-------------\n",
      "Television adevertising is on every channel.\n",
      "-------------\n",
      "But in the end it should not be forbidden to watch TV but it should be very rare like once a week or something like that.\n",
      "-------------\n",
      "For that parents can just install a code for those channels so the children will not have a chance to watch these.\n",
      "-------------\n",
      "If you think television adds are bad for your child, just don't let it watch tv, what it doesn't know, it doesn't miss.\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "for i,t in enumerate(df_clusters.query(f\"cluster == {0}\").sample(10).iterrows()):\n",
    "    print(t[1][\"text\"])\n",
    "    print(\"-------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "df_clusters.to_csv('clusteredArgument.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "(38715, 3)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clusters.shape\n",
    "\n",
    "# df_clusters.tokens[0]\n",
    "# df_clusters.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Most representative clusters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23971\n",
      "In my opinion it is not good to let the children watch TV everytime but the solution is not to just forbid it because lots of children like to watch TV and for example in the morning when everyone is still sleeping and when it is cold outside the children are glad to watch TV so they can pass a little time and it is also easier for the parents so they are able to sleep longer.\n",
      "-------------\n",
      "25361\n",
      "And if people think that they're kids are getting brainwashed, then maybe they shouldn't put them right in front of the TV.\n",
      "-------------\n",
      "21924\n",
      "Children who watch too much TV do not care anymore about their real lifewith their true friends and prefer to watch Tv than to go out.These people may have a big disadvantage in their lives.\n",
      "-------------\n",
      "10737\n",
      "So, we just decided that watching TV can sometimes be helpful even when itÂ´s not that healthy.\n",
      "-------------\n",
      "36675\n",
      "Because children have enough imagination for other activities if you let them think and don't put them in front of the Tv where they lose their ability to think about a game.\n",
      "-------------\n",
      "29010\n",
      "We could argue, that it is better if the children don't watch any TV at all or that kids these days don't watch on the classical TV screen and rather are surfing the internet, but that's beside the point.\n",
      "-------------\n",
      "31267\n",
      "In this generation it's normal that young children watch a lot of TV all day, because it's the easiest thing: they have something to do and don't make any noises.\n",
      "-------------\n",
      "8178\n",
      "When they watch TV at this age they maybe get used to it and always watch television.This has a big effect on their future, because it's not healthy to not move or do other stuff for example homework and just watch television.\n",
      "-------------\n",
      "15227\n",
      "I grew up without watching TV and I do not think that this did any harm to me.\n",
      "-------------\n",
      "15285\n",
      "Especially the television is a medium wich young children like very much, they spent a lot of time in front of it and think that everything on this screen must be the truth an a very good thing.\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "test_cluster = 0\n",
    "most_representative_docs = np.argsort(\n",
    "    np.linalg.norm(vectorized_docs - clustering.cluster_centers_[test_cluster], axis=1)\n",
    ")\n",
    "# print(most_representative_docs[0])\n",
    "for d in most_representative_docs[:10]:\n",
    "    print(d)\n",
    "    print(sentences[d])\n",
    "    print(\"-------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "array([6])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#array = clustering.cluster_centers_[0]\n",
    "#print(len(vectorized_docs))\n",
    "#print(array)\n",
    "array = vectorized_docs[120].reshape(1,-1)\n",
    "convertedArray = array.astype(float)\n",
    "# vectorized_docs[i] = sentences[i] it is the same\n",
    "clustering.predict(convertedArray)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.16868031 -0.11518344  0.04375385 -0.16961181 -0.32910988  0.07519562\n",
      "  -0.03473708  0.2881326  -0.22865133 -0.30180973 -0.2749814   0.05678155\n",
      "   0.22902955  0.25514063 -0.08736596 -0.4990135  -0.0990077  -0.01215302\n",
      "  -0.6290117  -0.21519752  0.07409988  0.7055213   0.22721244  0.12753491\n",
      "  -0.03387943  0.01707247 -0.2618402   0.33965072 -0.17506336  0.1853242\n",
      "  -0.40236542 -0.16700786  0.3402451  -0.11415518 -0.32711414 -0.12293297\n",
      "   0.32198665 -0.3418258   0.10985515  0.17799467  0.2483626  -0.08238377\n",
      "   0.2432443  -0.50477004  0.32689932 -0.09225568  0.0034604   0.32537356\n",
      "   0.67596817 -0.65887624  0.18741415 -0.30047625 -0.18230766  0.08700434\n",
      "   0.10143799  0.18505335 -0.13225521  0.24774665 -0.10696658 -0.04113882\n",
      "  -0.32722214 -0.33064002 -0.24776226  0.09273881  0.3967111  -0.05888849\n",
      "   0.14747654 -0.1701913  -0.14426146  0.06001846  0.2656059   0.10673421\n",
      "  -0.04613359 -0.6488152   0.15446118 -0.22528869  0.26844132 -0.23360653\n",
      "  -0.22219734 -0.25563705  0.04363551  0.00369294 -0.3977344  -0.47572312\n",
      "  -0.26682305  0.2726812   0.2365431  -0.9263358  -0.3509877   0.04909198\n",
      "  -0.9681525   0.26646307 -0.00749991 -0.12641941  0.23628266  0.10552181\n",
      "   0.23498236  0.601182   -0.28532207  0.0415365 ]]\n",
      "[[ 0.16868031 -0.11518344  0.04375385 -0.16961181 -0.32910988  0.07519562\n",
      "  -0.03473708  0.28813261 -0.22865133 -0.30180973 -0.27498141  0.05678155\n",
      "   0.22902955  0.25514063 -0.08736596 -0.49901351 -0.0990077  -0.01215302\n",
      "  -0.62901169 -0.21519752  0.07409988  0.70552129  0.22721244  0.12753491\n",
      "  -0.03387943  0.01707247 -0.26184019  0.33965072 -0.17506336  0.18532421\n",
      "  -0.40236542 -0.16700786  0.3402451  -0.11415518 -0.32711414 -0.12293297\n",
      "   0.32198665 -0.34182581  0.10985515  0.17799467  0.2483626  -0.08238377\n",
      "   0.24324431 -0.50477004  0.32689932 -0.09225568  0.0034604   0.32537356\n",
      "   0.67596817 -0.65887624  0.18741415 -0.30047625 -0.18230766  0.08700434\n",
      "   0.10143799  0.18505335 -0.13225521  0.24774665 -0.10696658 -0.04113882\n",
      "  -0.32722214 -0.33064002 -0.24776226  0.09273881  0.39671111 -0.05888849\n",
      "   0.14747654 -0.1701913  -0.14426146  0.06001846  0.2656059   0.10673421\n",
      "  -0.04613359 -0.64881521  0.15446118 -0.22528869  0.26844132 -0.23360653\n",
      "  -0.22219734 -0.25563705  0.04363551  0.00369294 -0.3977344  -0.47572312\n",
      "  -0.26682305  0.27268121  0.2365431  -0.92633581 -0.3509877   0.04909198\n",
      "  -0.96815252  0.26646307 -0.00749991 -0.12641941  0.23628266  0.10552181\n",
      "   0.23498236  0.60118198 -0.28532207  0.0415365 ]]\n"
     ]
    }
   ],
   "source": [
    "#print(array.reshape(1,-1))\n",
    "\n",
    "print(array)\n",
    "\n",
    "print(convertedArray)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predicting new clusters for testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "## testing\n",
    "def vectorizeSentenceTest(sentences):\n",
    "    tokensSentenceslist = []\n",
    "    for s  in sentences:\n",
    "        wordsList = word_tokenize(s)\n",
    "        tokensSentenceslist.append(wordsList)\n",
    "    return tokensSentenceslist\n",
    "\n",
    "testTokens = vectorizeSentenceTest([\"Watching tv for long hours leads to lazness\"])\n",
    "\n",
    "vectorized_docs_tesing = vectorize(testTokens, model=model, strategy=\"average\")\n",
    "\n",
    "def predictTest(vectorizedDocsTest):\n",
    "    array = vectorizedDocsTest\n",
    "    print(clustering.predict(array))\n",
    "    return\n",
    "#len(vectorized_docs_tesing), len(vectorized_docs_tesing[0])\n",
    "#print(vectorized_docs_tesing)\n",
    "\n",
    "predictTest(vectorized_docs_tesing)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list\n",
      "(38715,)\n"
     ]
    }
   ],
   "source": [
    "from Cython import typeof\n",
    "\n",
    "print(typeof(vectorized_docs))\n",
    "\n",
    "# print(vectorized_docs[0])\n",
    "# print(vectorized_docs[0].shape)\n",
    "# print (\"############################################\")\n",
    "#\n",
    "# print(clustering.cluster_centers_[0])\n",
    "# print(clustering.cluster_centers_[0].shape)\n",
    "print (most_representative_docs.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "itemindex = np.where(vectorized_docs == clustering.cluster_centers_[0])\n",
    "\n",
    "print(itemindex)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "testDocs = np.sort(\n",
    "    np.linalg.norm(vectorized_docs - clustering.cluster_centers_[0], axis=1)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.12176871 -0.79313385 -0.00996342 -0.24011901 -0.24725829  0.19357361\n",
      " -0.46350023  0.23181807 -0.21608464  0.12644906  0.04430683  0.75692064\n",
      " -0.07661071  1.2821914   0.3033127   0.27832875  0.19656463 -1.1782662\n",
      " -0.41312933 -0.08160181 -0.48066178  0.05651013 -0.23837197  0.36745435\n",
      "  1.1551768   0.14768282 -0.11665241  0.10531715 -0.42095342  0.1757475\n",
      " -0.30491138 -0.2725099  -0.24853076  0.20087126  0.8452291  -0.2359287\n",
      " -0.3064599  -1.2386026  -0.09007625  0.20649533 -0.11888672  0.23353544\n",
      "  0.75751346 -0.21393278  0.21745685 -0.2003168   0.13339151  0.92616767\n",
      "  0.73504096 -0.2729033 ]\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_docs[16035])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing wether the centroid are sentences or not"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "itemindex = np.where(vectorized_docs == clustering.cluster_centers_[9])\n",
    "\n",
    "print(itemindex[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}