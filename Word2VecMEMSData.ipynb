{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Importing section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from nltk import tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from nltk.corpus import stopwords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/690 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e5d57bcfd374e1981ce3d5ec8eb5870"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45d325bdd495442193b6e3936d8220c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/3.99k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2cedfdd0d3d4df4accd3426fb785b5f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/550 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5e01d05ed8e459580a1323261c59acc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "148477b7952e4b3a878c8ca1f245e655"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/265M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "482ceaacf2fc4f70872781d891f44015"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "201760a0268e4158a4183ffc23d8bb1e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "baa3ea1a27864310a1a5b7da7837793b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0bf17ab8a79045428b74cc5aac246d5b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/450 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86c04b4cf5f44fcebdefe7f2a9e40ad4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "727d923cb03642e18f54d89f28f8fc9c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd0458a5723e440187c0257217c92dec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedder = SentenceTransformer('distilbert-base-nli-mean-tokens')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "corpus = ['A man is eating food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'A man is eating pasta.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'The baby is carried by the woman',\n",
    "          'A man is riding a horse.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'Someone in a gorilla costume is playing a set of drums.',\n",
    "          'A cheetah is running behind its prey.',\n",
    "          'A cheetah chases prey on across a field.'\n",
    "          ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "corpus_embeddings = embedder.encode(corpus)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "num_clusters = 5\n",
    "clustering_model = KMeans(n_clusters=num_clusters)\n",
    "clustering_model.fit(corpus_embeddings)\n",
    "cluster_assignment = clustering_model.labels_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "clustered_sentences = [[] for i in range(num_clusters)]\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    clustered_sentences[cluster_id].append(corpus[sentence_id])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  1\n",
      "['A monkey is playing drums.', 'Someone in a gorilla costume is playing a set of drums.']\n",
      "\n",
      "Cluster  2\n",
      "['A man is eating food.', 'A man is eating a piece of bread.', 'A man is eating pasta.']\n",
      "\n",
      "Cluster  3\n",
      "['A man is riding a horse.', 'A man is riding a white horse on an enclosed ground.']\n",
      "\n",
      "Cluster  4\n",
      "['The girl is carrying a baby.', 'The baby is carried by the woman']\n",
      "\n",
      "Cluster  5\n",
      "['A cheetah is running behind its prey.', 'A cheetah chases prey on across a field.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, cluster in enumerate(clustered_sentences):\n",
    "    print(\"Cluster \", i+1)\n",
    "    print(cluster)\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reading the data from the folder and cleaning it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for filename in os.listdir(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\TelevisonMergedT1+T2\"):\n",
    "   with open(os.path.join(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\TelevisonMergedT1+T2\", filename)) as f:\n",
    "       text = f.read()\n",
    "       text = text.replace(\"ï»¿\",\"\")\n",
    "       sents = tokenize.sent_tokenize(text)\n",
    "       for s in sents:\n",
    "           #s = s.lower()\n",
    "           #s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "           sentences.append(s)\n",
    "\n",
    "tokensSentenceslist = []\n",
    "\n",
    "for s  in sentences:\n",
    "    wordsList = gensim.utils.simple_preprocess(s)\n",
    "    filtered_words = [word for word in wordsList if word not in stopwords.words('english')]\n",
    "    tokensSentenceslist.append(filtered_words)\n",
    "\n",
    "\n",
    "##################### Uncomment below section for testing #########################\n",
    "# print(len(sentences))\n",
    "#\n",
    "# for s in sentences:\n",
    "#      print(\"The sentence is : \")\n",
    "#      print(s)\n",
    "#      print(\"-----------------------End of the sentence -------------\")\n",
    "#\n",
    "# print (sentences)\n",
    "\n",
    "\n",
    "# print (len(tokensSentenceslist))\n",
    "# print (tokensSentenceslist)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['advertising', 'television', 'becoming', 'clever']\n"
     ]
    }
   ],
   "source": [
    "print (tokensSentenceslist[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'just', 'test', 'and', 'don', 'know']\n"
     ]
    }
   ],
   "source": [
    "testSentences = gensim.utils.simple_preprocess(\"This is just a test and i don't know\")\n",
    "\n",
    "print (testSentences)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38715\n",
      "['Advertising in Television is becoming more and more clever.', 'Lots of those ads invade our subconsciousness and try to make us buy the product even if we do not actually need it.', \"When it comes to children, lots of them don't have enough money to afford these products.\", \"They don't even understand the concept of money.\", 'Even if they wanted it, they can not get the thing they were told they need.']\n"
     ]
    }
   ],
   "source": [
    "print (len(sentences))\n",
    "print(sentences[0:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Generating the Word2Vec Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "(1313567, 1737390)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = Word2Vec(tokensSentenceslist, min_count=1)\n",
    "\n",
    "#model = Word2Vec(tokensSentenceslist, vector_size=50, min_count=1, sg=1)\n",
    "#model = Word2Vec(sentences=tokensSentenceslist, vector_size=100, workers=1, seed=42)\n",
    "\n",
    "model = Word2Vec(window=10, min_count=2,workers=6,vector_size=100,seed=42,sg=0)\n",
    "model.build_vocab(tokensSentenceslist, progress_per=1000)\n",
    "model.train(tokensSentenceslist, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "\n",
    "##################### Uncomment below section for testing #########################\n",
    "\n",
    "\n",
    "# print(list(model.wv.index_to_key))\n",
    "# print(len(list(model.wv.index_to_key)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "38715"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count\n",
    "#model.epochs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "[('reason', 0.9420978426933289),\n ('point', 0.9409478902816772),\n ('side', 0.9202869534492493),\n ('agree', 0.8974602818489075),\n ('aspect', 0.8958882093429565),\n ('reasons', 0.8917336463928223),\n ('support', 0.8810402154922485),\n ('personally', 0.8794190287590027),\n ('disagree', 0.8751274943351746),\n ('points', 0.8721221089363098)]"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.wv.most_similar(\"television\")\n",
    "model.wv.most_similar(\"argument\")\n",
    "#model.wv.similarity(\"tv\",\"television\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vectorizing each sentence using the avg of the Word embidings of each word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def vectorize(list_of_docs, model, strategy):\n",
    "    \"\"\"Generate vectors for list of documents using a Word Emx`bedding.\n",
    "\n",
    "    Args:\n",
    "        list_of_docs: List of documents.\n",
    "        model: Gensim Word Embedding.\n",
    "        strategy: Aggregation strategy (\"average\", or \"min-max\".)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the strategy is other than \"average\" or \"min-max\".\n",
    "\n",
    "    Returns:\n",
    "        List of vectors.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    size_output = model.vector_size\n",
    "    embedding_dict = model\n",
    "\n",
    "    if strategy == \"min-max\":\n",
    "        size_output *= 2\n",
    "\n",
    "    if hasattr(model, \"wv\"):\n",
    "        embedding_dict = model.wv\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(size_output)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in embedding_dict:\n",
    "                try:\n",
    "                    vectors.append(embedding_dict[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            if strategy == \"min-max\":\n",
    "                min_vec = vectors.min(axis=0)\n",
    "                max_vec = vectors.max(axis=0)\n",
    "                features.append(np.concatenate((min_vec, max_vec)))\n",
    "            elif strategy == \"average\":\n",
    "                avg_vec = vectors.mean(axis=0)\n",
    "                features.append(avg_vec)\n",
    "            else:\n",
    "                raise ValueError(f\"Aggregation strategy {strategy} does not exist!\")\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Apply the function above"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "vectorized_docs = vectorize(tokensSentenceslist, model=model, strategy=\"average\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38715 100\n",
      "[ 0.14049838  0.26483944 -0.31178233  0.04889421 -0.4739972   0.7198708\n",
      " -0.45431072  1.288989   -0.8510209  -0.12554932  0.16687194  0.74362314\n",
      "  0.53374034  0.13064659 -0.07579185 -0.48557985 -0.23639245 -1.2327642\n",
      " -0.43592197  0.48491108 -1.0318269  -0.54556245  0.5922161   0.38107365\n",
      "  0.540607    0.5871719  -0.15847757 -0.05579238 -0.326022   -0.9108644\n",
      " -0.06940957  0.16453063 -0.01091491 -0.40957034  0.45949882 -0.33499575\n",
      "  0.37234515 -0.44981864 -0.2568936   0.5194661   0.32487923 -0.8094511\n",
      "  0.45209676 -0.0290043   0.12825406  0.5299665   0.19131848  0.63766295\n",
      "  1.0680393  -0.7605646   0.39149797 -0.67647886 -0.9543225  -0.27332538\n",
      "  0.5363619   0.27479282  0.1006263  -0.05415126 -0.6557303   0.17720665\n",
      " -0.5534491   0.67129415 -1.1589129   0.18394628 -0.16723034 -0.38107216\n",
      "  0.28851694 -0.10897604 -1.6831535  -0.22472814  0.6801643  -0.63879627\n",
      " -0.02100011 -0.24930024  0.6857125   0.03476049 -1.1521822  -1.1853676\n",
      "  0.18162669 -0.25959158  0.11647352 -0.7353032  -0.31845513  0.4398339\n",
      " -0.40987632  0.03088767 -0.6767581  -0.6587676   0.07472274 -0.22439212\n",
      "  0.219106    0.9402928  -0.09816226  0.35060728 -0.152755   -0.41230875\n",
      "  0.4680558   0.04682444 -0.5024302  -0.19368286]\n",
      "#######################################################\n",
      "[ 1.65836006e-01  1.78595915e-01  4.40565199e-02 -1.84827060e-01\n",
      " -1.81415267e-02  3.24446052e-01  9.97339040e-02  5.80683589e-01\n",
      " -4.05189514e-01 -3.57736021e-01 -2.88448870e-01  5.15905619e-01\n",
      "  1.25453711e-01  2.46026143e-01 -4.77009788e-02 -4.43047345e-01\n",
      " -3.01382598e-02 -6.58109069e-01 -6.09308064e-01  2.01329872e-01\n",
      " -1.44826099e-01 -2.27841839e-01 -9.08073224e-03  4.76604164e-01\n",
      "  4.69488144e-01  2.35357523e-01 -1.84533522e-01  3.69239300e-01\n",
      " -1.50739104e-01 -8.69230703e-02 -1.23274207e-01  8.32623541e-02\n",
      " -4.09822911e-01 -7.45186508e-02 -3.81999984e-02 -6.16811216e-04\n",
      "  2.95882314e-01 -6.80425942e-01 -6.44816935e-01  3.19506973e-02\n",
      "  1.53549403e-01 -3.11181247e-01  2.70364806e-02 -3.00766855e-01\n",
      "  1.55999541e-01  1.45592093e-01  1.41220257e-01  1.17124483e-01\n",
      "  5.79970181e-01 -5.81747055e-01  2.05664247e-01 -5.62467217e-01\n",
      " -2.06434339e-01 -2.99396694e-01  1.34646431e-01 -9.98454168e-02\n",
      " -1.04375236e-01  2.20306024e-01 -1.73164219e-01  3.64866555e-01\n",
      " -4.79425117e-02  7.54573494e-02 -8.45797420e-01  3.42652678e-01\n",
      "  2.51532406e-01  2.49449611e-02  2.17947707e-01 -5.38597822e-01\n",
      " -6.94590032e-01 -5.40850721e-02 -1.76109314e-01 -3.78941208e-01\n",
      " -1.38950586e-01  1.54729057e-02  2.89655477e-01 -3.79414499e-01\n",
      " -2.06453040e-01 -4.73214984e-02 -1.45029813e-01 -1.48595050e-01\n",
      "  1.87607035e-02 -3.33082795e-01 -4.15420681e-01 -2.04829350e-02\n",
      " -4.38044280e-01  4.56361979e-01 -6.09595299e-01 -4.39447999e-01\n",
      " -8.45042467e-02  1.19384937e-01 -2.33661205e-01  3.10297966e-01\n",
      "  2.39110738e-02 -1.15536295e-01  5.18205822e-01  5.71607761e-02\n",
      " -1.26548052e-01  9.58031863e-02 -5.12107134e-01 -5.35919368e-02]\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorized_docs), len(vectorized_docs[0]))\n",
    "print(model.wv[\"argument\"])\n",
    "\n",
    "print(\"#######################################################\")\n",
    "print(vectorized_docs[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Kmeans algorithm with mini batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def mbkmeans_clusters(X, k, mb=500, print_silhouette_values=False):\n",
    "    \"\"\"Generate clusters.\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches. Defaults to 500.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Applying the Kmeans algorithm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 10\n",
      "Silhouette coefficient: 0.13\n",
      "Inertia:102984.12434970376\n",
      "Silhouette values:\n",
      "    Cluster 8: Size:163 | Avg:1.00 | Min:0.85 | Max: 1.00\n",
      "    Cluster 5: Size:492 | Avg:0.54 | Min:0.07 | Max: 0.59\n",
      "    Cluster 7: Size:5798 | Avg:0.21 | Min:0.01 | Max: 0.44\n",
      "    Cluster 0: Size:4777 | Avg:0.14 | Min:-0.13 | Max: 0.39\n",
      "    Cluster 9: Size:6689 | Avg:0.11 | Min:-0.06 | Max: 0.32\n",
      "    Cluster 2: Size:7661 | Avg:0.11 | Min:-0.08 | Max: 0.31\n",
      "    Cluster 3: Size:2207 | Avg:0.11 | Min:-0.20 | Max: 0.33\n",
      "    Cluster 4: Size:4894 | Avg:0.11 | Min:-0.11 | Max: 0.32\n",
      "    Cluster 1: Size:2895 | Avg:0.09 | Min:-0.11 | Max: 0.31\n",
      "    Cluster 6: Size:3139 | Avg:0.03 | Min:-0.18 | Max: 0.28\n"
     ]
    }
   ],
   "source": [
    "clustering, cluster_labels = mbkmeans_clusters(X=vectorized_docs, k=10, print_silhouette_values=True)\n",
    "\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"text\": sentences,\n",
    "    \"tokens\": [\" \".join(text) for text in tokensSentenceslist],\n",
    "    \"cluster\": cluster_labels\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate top terms of the cluster"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Top terms per cluster (based on centroids):\")\n",
    "for i in range(10): # number of cluster k should be put here!!\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[i]], topn=10)\n",
    "    #print(clustering.cluster_centers_[i])\n",
    "    for t in most_representative:\n",
    "        tokens_per_cluster += f\"{t[0]} \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster (based on centroids):\n",
      "Cluster 0: anyway letting donâ okey sometimes childen shouldnâ put controll whatch \n",
      "Cluster 1: probably afford tell best mad advert disappoint want course sad \n",
      "Cluster 2: advertises childre firstly necessary difference however thats personaly affected desicide \n",
      "Cluster 3: allowed forbidden nutshell opinion schould childern difficult question tein allowd \n",
      "Cluster 4: childern schould personaly difficult shouldnâ nutshell oppinion sense adversting necessary \n",
      "Cluster 5: versuchen sie zeit stuffed shoes die hair wie fantasies app \n",
      "Cluster 6: addicted others thy nothing helpful behave doesnâ forget better healthy \n",
      "Cluster 7: partents possibility addition wont pretty wonâ wich worse thought simply \n",
      "Cluster 8: anhand wird ihr beurteilt qualitã carrot ten shouts boredom speaker \n",
      "Cluster 9: case course furthermore cannot useful realise anything worse actually mean \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: tv(2584) children(2273) watch(2252) television(1513) watching(988) \n",
      "Cluster 1: buy(1308) parents(1262) want(1101) toy(696) child(664) \n",
      "Cluster 2: children(3768) advertising(2152) young(1449) television(1434) think(927) \n",
      "Cluster 3: children(1625) young(1358) advertising(1345) television(1179) directed(1177) \n",
      "Cluster 4: children(3905) young(2465) advertising(2239) television(1856) directed(1048) \n",
      "Cluster 5: sasageyo(1092) die(654) zeit(325) sie(324) diese(177) \n",
      "Cluster 6: learn(966) children(964) things(634) play(530) life(528) \n",
      "Cluster 7: children(586) like(535) child(463) example(432) television(350) \n",
      "Cluster 8: ihr(163) text(163) wird(163) anhand(163) beurteilt(163) \n",
      "Cluster 9: children(3087) parents(2020) want(1391) get(1048) things(1042) \n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for i in range(10):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_frequent = Counter(\" \".join(df_clusters.query(f\"cluster == {i}\")[\"tokens\"]).split()).most_common(5)\n",
    "    for t in most_frequent:\n",
    "        tokens_per_cluster += f\"{t[0]}({str(t[1])}) \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Retrieve a random sample of documents for a given cluster\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The problem is that children from two to five arenÂ´t even able to read in this age so they start watching TV before they are able to read a book.\n",
      "-------------\n",
      "If they are constantly confronted with something they like they wonÂ´t stop watching it causing them to be addicted to television  which is bad for a young child that should grow.\n",
      "-------------\n",
      "So I think that it is important that the parents show their kids films which are appropriated too the childrens ages.\n",
      "-------------\n",
      "First of all, it could be a problem if the children don't watch television.\n",
      "-------------\n",
      "It also depends on what time it is.\n",
      "-------------\n",
      "Another problem, however is their increasing time in front of the TV.\n",
      "-------------\n",
      "It's all much more important than showing them how to turn on a tv.\n",
      "-------------\n",
      "Kids from young age on get used to a TV, a mobile phone and the computer.\n",
      "-------------\n",
      "In fact we have to  ask ourselves why a five year old children would watch that much tv that they see some advertisement of a product they want?It is not good for children to send so much time inside watching television.\n",
      "-------------\n",
      "Another negativ point ist that their are sometimes TV shows wich are not realistic or just nothing for childs at the age of two to five but the parents let the kids watch this.\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "for i,t in enumerate(df_clusters.query(f\"cluster == {0}\").sample(10).iterrows()):\n",
    "    print(t[1][\"text\"])\n",
    "    print(\"-------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "df_clusters.to_csv('clusteredArgument.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "(38715, 3)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clusters.shape\n",
    "\n",
    "# df_clusters.tokens[0]\n",
    "# df_clusters.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Most representative clusters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23971\n",
      "In my opinion it is not good to let the children watch TV everytime but the solution is not to just forbid it because lots of children like to watch TV and for example in the morning when everyone is still sleeping and when it is cold outside the children are glad to watch TV so they can pass a little time and it is also easier for the parents so they are able to sleep longer.\n",
      "-------------\n",
      "11348\n",
      "There is no doubt that it isnÂ´t that good for young children to watch TV because they can get sick or they only want to do this all day long.\n",
      "-------------\n",
      "35281\n",
      "However nowadays there are too many parents not being able to handle parenting and just letting their children watch tv all day.\n",
      "-------------\n",
      "36914\n",
      "That means that parents have to know what their children watch and when series are on the TV specially produced for childrens learning.\n",
      "-------------\n",
      "17941\n",
      "I think itÂ´s ok when children watch Tv, but the parent must controll the programs and the time.\n",
      "-------------\n",
      "31267\n",
      "In this generation it's normal that young children watch a lot of TV all day, because it's the easiest thing: they have something to do and don't make any noises.\n",
      "-------------\n",
      "36675\n",
      "Because children have enough imagination for other activities if you let them think and don't put them in front of the Tv where they lose their ability to think about a game.\n",
      "-------------\n",
      "25361\n",
      "And if people think that they're kids are getting brainwashed, then maybe they shouldn't put them right in front of the TV.\n",
      "-------------\n",
      "29074\n",
      "But if the children start watching TV  for consecutive hours,days or even weeks I think it's not good for them.\n",
      "-------------\n",
      "3510\n",
      "Maybe they go secretly in the livingroom to watch TV and if this is the cause the children would watch television for adults and no program for them.\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "test_cluster = 0\n",
    "most_representative_docs = np.argsort(\n",
    "    np.linalg.norm(vectorized_docs - clustering.cluster_centers_[test_cluster], axis=1)\n",
    ")\n",
    "# print(most_representative_docs[0])\n",
    "for d in most_representative_docs[:10]:\n",
    "    print(d)\n",
    "    print(sentences[d])\n",
    "    print(\"-------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "array([6])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#array = clustering.cluster_centers_[0]\n",
    "#print(len(vectorized_docs))\n",
    "#print(array)\n",
    "array = vectorized_docs[120].reshape(1,-1)\n",
    "convertedArray = array.astype(float)\n",
    "# vectorized_docs[i] = sentences[i] it is the same\n",
    "clustering.predict(convertedArray)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.16868031 -0.11518344  0.04375385 -0.16961181 -0.32910988  0.07519562\n",
      "  -0.03473708  0.2881326  -0.22865133 -0.30180973 -0.2749814   0.05678155\n",
      "   0.22902955  0.25514063 -0.08736596 -0.4990135  -0.0990077  -0.01215302\n",
      "  -0.6290117  -0.21519752  0.07409988  0.7055213   0.22721244  0.12753491\n",
      "  -0.03387943  0.01707247 -0.2618402   0.33965072 -0.17506336  0.1853242\n",
      "  -0.40236542 -0.16700786  0.3402451  -0.11415518 -0.32711414 -0.12293297\n",
      "   0.32198665 -0.3418258   0.10985515  0.17799467  0.2483626  -0.08238377\n",
      "   0.2432443  -0.50477004  0.32689932 -0.09225568  0.0034604   0.32537356\n",
      "   0.67596817 -0.65887624  0.18741415 -0.30047625 -0.18230766  0.08700434\n",
      "   0.10143799  0.18505335 -0.13225521  0.24774665 -0.10696658 -0.04113882\n",
      "  -0.32722214 -0.33064002 -0.24776226  0.09273881  0.3967111  -0.05888849\n",
      "   0.14747654 -0.1701913  -0.14426146  0.06001846  0.2656059   0.10673421\n",
      "  -0.04613359 -0.6488152   0.15446118 -0.22528869  0.26844132 -0.23360653\n",
      "  -0.22219734 -0.25563705  0.04363551  0.00369294 -0.3977344  -0.47572312\n",
      "  -0.26682305  0.2726812   0.2365431  -0.9263358  -0.3509877   0.04909198\n",
      "  -0.9681525   0.26646307 -0.00749991 -0.12641941  0.23628266  0.10552181\n",
      "   0.23498236  0.601182   -0.28532207  0.0415365 ]]\n",
      "[[ 0.16868031 -0.11518344  0.04375385 -0.16961181 -0.32910988  0.07519562\n",
      "  -0.03473708  0.28813261 -0.22865133 -0.30180973 -0.27498141  0.05678155\n",
      "   0.22902955  0.25514063 -0.08736596 -0.49901351 -0.0990077  -0.01215302\n",
      "  -0.62901169 -0.21519752  0.07409988  0.70552129  0.22721244  0.12753491\n",
      "  -0.03387943  0.01707247 -0.26184019  0.33965072 -0.17506336  0.18532421\n",
      "  -0.40236542 -0.16700786  0.3402451  -0.11415518 -0.32711414 -0.12293297\n",
      "   0.32198665 -0.34182581  0.10985515  0.17799467  0.2483626  -0.08238377\n",
      "   0.24324431 -0.50477004  0.32689932 -0.09225568  0.0034604   0.32537356\n",
      "   0.67596817 -0.65887624  0.18741415 -0.30047625 -0.18230766  0.08700434\n",
      "   0.10143799  0.18505335 -0.13225521  0.24774665 -0.10696658 -0.04113882\n",
      "  -0.32722214 -0.33064002 -0.24776226  0.09273881  0.39671111 -0.05888849\n",
      "   0.14747654 -0.1701913  -0.14426146  0.06001846  0.2656059   0.10673421\n",
      "  -0.04613359 -0.64881521  0.15446118 -0.22528869  0.26844132 -0.23360653\n",
      "  -0.22219734 -0.25563705  0.04363551  0.00369294 -0.3977344  -0.47572312\n",
      "  -0.26682305  0.27268121  0.2365431  -0.92633581 -0.3509877   0.04909198\n",
      "  -0.96815252  0.26646307 -0.00749991 -0.12641941  0.23628266  0.10552181\n",
      "   0.23498236  0.60118198 -0.28532207  0.0415365 ]]\n"
     ]
    }
   ],
   "source": [
    "#print(array.reshape(1,-1))\n",
    "\n",
    "print(array)\n",
    "\n",
    "print(convertedArray)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predicting new clusters for testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "## testing\n",
    "def vectorizeSentenceTest(sentences):\n",
    "    tokensSentenceslist = []\n",
    "    for s  in sentences:\n",
    "        wordsList = word_tokenize(s)\n",
    "        tokensSentenceslist.append(wordsList)\n",
    "    return tokensSentenceslist\n",
    "\n",
    "testTokens = vectorizeSentenceTest([\"Buying some products can be very expensive\"])\n",
    "\n",
    "vectorized_docs_tesing = vectorize(testTokens, model=model, strategy=\"average\")\n",
    "\n",
    "def predictTest(vectorizedDocsTest):\n",
    "    array = vectorizedDocsTest\n",
    "    print(clustering.predict(array))\n",
    "    return\n",
    "#len(vectorized_docs_tesing), len(vectorized_docs_tesing[0])\n",
    "#print(vectorized_docs_tesing)\n",
    "\n",
    "predictTest(vectorized_docs_tesing)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list\n",
      "(38715,)\n"
     ]
    }
   ],
   "source": [
    "from Cython import typeof\n",
    "\n",
    "print(typeof(vectorized_docs))\n",
    "\n",
    "# print(vectorized_docs[0])\n",
    "# print(vectorized_docs[0].shape)\n",
    "# print (\"############################################\")\n",
    "#\n",
    "# print(clustering.cluster_centers_[0])\n",
    "# print(clustering.cluster_centers_[0].shape)\n",
    "print (most_representative_docs.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "itemindex = np.where(vectorized_docs == clustering.cluster_centers_[0])\n",
    "\n",
    "print(itemindex)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "testDocs = np.sort(\n",
    "    np.linalg.norm(vectorized_docs - clustering.cluster_centers_[0], axis=1)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.12176871 -0.79313385 -0.00996342 -0.24011901 -0.24725829  0.19357361\n",
      " -0.46350023  0.23181807 -0.21608464  0.12644906  0.04430683  0.75692064\n",
      " -0.07661071  1.2821914   0.3033127   0.27832875  0.19656463 -1.1782662\n",
      " -0.41312933 -0.08160181 -0.48066178  0.05651013 -0.23837197  0.36745435\n",
      "  1.1551768   0.14768282 -0.11665241  0.10531715 -0.42095342  0.1757475\n",
      " -0.30491138 -0.2725099  -0.24853076  0.20087126  0.8452291  -0.2359287\n",
      " -0.3064599  -1.2386026  -0.09007625  0.20649533 -0.11888672  0.23353544\n",
      "  0.75751346 -0.21393278  0.21745685 -0.2003168   0.13339151  0.92616767\n",
      "  0.73504096 -0.2729033 ]\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_docs[16035])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing wether the centroid are sentences or not"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "itemindex = np.where(vectorized_docs == clustering.cluster_centers_[9])\n",
    "\n",
    "print(itemindex[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}