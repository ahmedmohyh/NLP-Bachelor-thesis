{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Importing section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from nltk import tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reading the data from the folder and cleaning it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for filename in os.listdir(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\TelevisonMergedT1+T2\"):\n",
    "   with open(os.path.join(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\TelevisonMergedT1+T2\", filename)) as f:\n",
    "       text = f.read()\n",
    "       text = text.replace(\"ï»¿\",\"\")\n",
    "       sents = tokenize.sent_tokenize(text)\n",
    "       for s in sents:\n",
    "           s = s.lower()\n",
    "           s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "           sentences.append(s)\n",
    "\n",
    "tokensSentenceslist = []\n",
    "\n",
    "for s  in sentences:\n",
    "    wordsList = word_tokenize(s)\n",
    "    tokensSentenceslist.append(wordsList)\n",
    "\n",
    "\n",
    "##################### Uncomment below section for testing #########################\n",
    "# print(len(sentences))\n",
    "#\n",
    "# for s in sentences:\n",
    "#      print(\"The sentence is : \")\n",
    "#      print(s)\n",
    "#      print(\"-----------------------End of the sentence -------------\")\n",
    "#\n",
    "# print (sentences)\n",
    "\n",
    "\n",
    "# print (len(tokensSentenceslist))\n",
    "# print (tokensSentenceslist)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38715\n",
      "['advertising in television is becoming more and more clever', 'lots of those ads invade our subconsciousness and try to make us buy the product even if we do not actually need it', 'when it comes to children lots of them dont have enough money to afford these products', 'they dont even understand the concept of money', 'even if they wanted it they can not get the thing they were told they need']\n"
     ]
    }
   ],
   "source": [
    "print (len(sentences))\n",
    "print(sentences[0:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generating the Word2Vec Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "(2373525, 3636460)"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = Word2Vec(tokensSentenceslist, min_count=1)\n",
    "\n",
    "#model = Word2Vec(tokensSentenceslist, vector_size=50, min_count=1, sg=1)\n",
    "#model = Word2Vec(sentences=tokensSentenceslist, vector_size=100, workers=1, seed=42)\n",
    "\n",
    "model = Word2Vec(window=10, min_count=2,workers=6,vector_size=50,seed=42,sg=0)\n",
    "model.build_vocab(tokensSentenceslist, progress_per=1000)\n",
    "model.train(tokensSentenceslist, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "\n",
    "##################### Uncomment below section for testing #########################\n",
    "\n",
    "\n",
    "# print(list(model.wv.index_to_key))\n",
    "# print(len(list(model.wv.index_to_key)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "38715"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count\n",
    "#model.epochs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"television\")\n",
    "model.wv.most_similar(\"argument\")\n",
    "model.wv.similarity(\"tv\",\"against\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "-0.1692446"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize(list_of_docs, model, strategy):\n",
    "    \"\"\"Generate vectors for list of documents using a Word Emx`bedding.\n",
    "\n",
    "    Args:\n",
    "        list_of_docs: List of documents.\n",
    "        model: Gensim Word Embedding.\n",
    "        strategy: Aggregation strategy (\"average\", or \"min-max\".)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the strategy is other than \"average\" or \"min-max\".\n",
    "\n",
    "    Returns:\n",
    "        List of vectors.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    size_output = model.vector_size\n",
    "    embedding_dict = model\n",
    "\n",
    "    if strategy == \"min-max\":\n",
    "        size_output *= 2\n",
    "\n",
    "    if hasattr(model, \"wv\"):\n",
    "        embedding_dict = model.wv\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(size_output)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in embedding_dict:\n",
    "                try:\n",
    "                    vectors.append(embedding_dict[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            if strategy == \"min-max\":\n",
    "                min_vec = vectors.min(axis=0)\n",
    "                max_vec = vectors.max(axis=0)\n",
    "                features.append(np.concatenate((min_vec, max_vec)))\n",
    "            elif strategy == \"average\":\n",
    "                avg_vec = vectors.mean(axis=0)\n",
    "                features.append(avg_vec)\n",
    "            else:\n",
    "                raise ValueError(f\"Aggregation strategy {strategy} does not exist!\")\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "vectorized_docs = vectorize(tokensSentenceslist, model=model, strategy=\"average\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.40850586 -0.34978685 -1.959126    2.6344159   0.8636176   0.73862237\n",
      " -0.01656396  2.1689315   0.09455873  2.232262    1.0606693   0.6851661\n",
      "  1.2025019  -0.62458533 -0.8933515   0.15206935  0.7622899   0.23542207\n",
      "  1.1568854  -0.5191893   1.1853704   0.35496783  1.3046758  -0.6329307\n",
      "  0.23321836 -0.18543202  1.7009146  -0.8653078   0.6799102   3.2721758\n",
      " -0.23107289  1.178943   -0.13785091 -2.7942522  -1.4669671  -1.5336914\n",
      " -0.1132482  -3.6796699   0.6580933   0.55389977  0.06629828 -0.5170109\n",
      "  1.0349716  -0.06024138 -1.1638317  -1.7385219   0.52351743  0.2860309\n",
      " -0.41214272 -0.5447228 ]\n",
      "#######################################################\n",
      "[ 0.41131788 -0.11716345  0.28971842  0.5577258  -0.5772017   0.9901962\n",
      "  0.96931005 -0.5613676  -0.3226952  -0.4045963   0.18726283  0.44057044\n",
      " -0.42001647 -0.4549887  -0.8095281   0.07320678 -0.20656103 -0.8354379\n",
      "  0.20628421 -0.6952043  -0.29465434 -0.11770947 -0.03002735  0.10196522\n",
      "  0.37290215 -0.10642844  0.10388153 -0.5029228  -0.48719856  0.58507514\n",
      " -0.10985485 -0.43433896 -0.43355358 -0.5003478  -0.255086    1.0371667\n",
      " -0.56815755 -1.0393796  -0.58956033  0.4535848  -0.05363839 -0.7777182\n",
      "  1.1643625  -0.07477699 -0.07372791 -0.9933421  -0.22898763 -0.10050163\n",
      " -0.13402921 -0.31821993]\n"
     ]
    }
   ],
   "source": [
    "len(vectorized_docs), len(vectorized_docs[0])\n",
    "print(model.wv[\"argument\"])\n",
    "\n",
    "print(\"#######################################################\")\n",
    "print(vectorized_docs[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.0834987   1.0775832  -2.4190793   1.3539352   0.7693121  -0.01766455\n",
      " -0.53748876  2.1623745  -0.5207145   1.694928   -0.2794287   1.4977117\n",
      "  0.3757558  -1.0392004  -1.1415775  -0.37531468  0.23239893  1.364053\n",
      "  1.6795251   0.6714833   0.8824193   1.365521    1.8032991  -0.67293257\n",
      "  1.0104337  -0.03604451  1.0017824  -0.5705173  -0.49769476  1.6430551\n",
      " -1.352539    1.2772088   0.03308525 -2.0387952  -2.4316406  -1.2589905\n",
      "  0.95908076 -2.2138608   0.6789476  -1.0684443   0.03440151  0.9796872\n",
      "  0.9881222  -1.3170159  -2.6712847  -1.2676587   1.5911027  -0.6523292\n",
      " -1.2096913   0.46780825]\n",
      "#######################################################\n",
      "[ 0.09301123  0.02892222  0.02476533  0.29560983 -0.7727718   0.23872662\n",
      "  0.4515218  -0.06879666 -0.03741706 -0.3008877  -0.20542604  0.8528051\n",
      " -0.14720449 -0.2436079  -1.0440915  -0.17934558 -0.21951494 -0.6070896\n",
      "  0.41599515  0.6486483  -0.45739534  0.38212645  0.3662845   0.4625144\n",
      "  0.98150975 -0.35466102  0.38813674 -0.44521755 -1.4364208  -0.07891957\n",
      " -0.056222   -0.19564575  0.38996303 -0.32186234  0.00216475  1.0545774\n",
      " -0.41077358 -0.5827029  -0.9488093   0.06095479 -0.050621   -0.20328291\n",
      "  0.8148782  -0.7642552  -0.32234916 -0.83504534 -0.48661792 -0.42191637\n",
      " -0.5468443  -0.08259413]\n"
     ]
    }
   ],
   "source": [
    "def mbkmeans_clusters(X, k, mb=500, print_silhouette_values=False):\n",
    "    \"\"\"Generate clusters.\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches. Defaults to 500.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "clustering, cluster_labels = mbkmeans_clusters(X=vectorized_docs, k=10, print_silhouette_values=True)\n",
    "\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"text\": sentences,\n",
    "    \"tokens\": [\" \".join(text) for text in tokensSentenceslist],\n",
    "    \"cluster\": cluster_labels\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate top terms of the cluster"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Top terms per cluster (based on centroids):\")\n",
    "for i in range(10): # number of cluster k should be put here!!\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[i]], topn=3)\n",
    "    print(clustering.cluster_centers_[i])\n",
    "    for t in most_representative:\n",
    "        tokens_per_cluster += f\"{t[0]} \"\n",
    "    #print(f\"Cluster {i}: {tokens_per_cluster}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 196,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster (based on centroids):\n",
      "[ 1.19385659 -0.82888172  1.58800147  0.10622866 -0.04769936  0.53634079\n",
      "  1.2926703  -0.58823001  0.37684249 -0.0375403   1.1391392  -0.21388091\n",
      " -0.8419543  -0.18153256  0.01115879  0.0577653  -0.66395227  0.1070669\n",
      "  0.3855902   0.07921372  0.25195381 -0.24987248  0.91514523  0.87854672\n",
      " -0.2781197  -1.14427267  0.05927777  0.0140876  -1.56937785  0.24000135\n",
      "  0.75462581 -0.68548434 -0.51476748  0.84428861 -0.97498851  0.26553429\n",
      "  0.41518204 -0.35250388 -1.04682077  0.19067908 -0.42692063  0.4745817\n",
      "  0.7416655  -0.27141488  0.15662372 -0.24953608  0.13406592  0.3687836\n",
      " -0.95627766  0.222207  ]\n",
      "[ 0.70034438 -0.29410066  0.99018799  0.20499822 -0.04243745  0.39972958\n",
      "  0.64275578 -0.33103261 -0.0416056  -0.13465635  0.72499667  0.00946553\n",
      " -0.46231743  0.12923565 -0.10479293 -0.02107713 -0.41081607 -0.20373062\n",
      "  0.31501559 -0.29613619  0.14241664 -0.22131774  0.29306809  0.58699511\n",
      "  0.02967619 -0.48050582  0.10401114  0.04557529 -1.1784371   0.21390013\n",
      "  0.45816505 -0.42654698 -0.42213327  0.23249377 -0.60067039 -0.00714601\n",
      "  0.06972742 -0.79242107 -0.32436566  0.31826471 -0.1903578   0.1888943\n",
      "  0.86861939 -0.07438913  0.00163146 -0.15929379  0.02261223  0.21485612\n",
      " -0.57459968 -0.08225413]\n",
      "[ 0.87680405  0.17769165 -0.06149382  0.10299494  0.21017745  0.38209173\n",
      "  0.66265316  0.03091055  0.08586377 -0.04961441  0.07367984  0.04637753\n",
      " -0.3364137   0.23556231  0.09801026  0.3410713  -0.33291734  0.46806827\n",
      "  0.56448401 -0.5281131   0.5040896  -0.48118904  0.8111509  -0.14780404\n",
      " -0.09267816 -0.66997189  0.24608964 -0.50303203 -0.79106131  0.51385704\n",
      "  0.16467694  0.29749151 -0.49938007  0.63981882 -1.21195506 -0.47856271\n",
      " -0.12154764 -0.63650021 -0.46953281  0.27908484  0.13773129 -0.44739879\n",
      "  0.73058978 -0.49077334 -0.61268438 -0.85654807  0.20647105  0.60050473\n",
      " -0.56216424  0.23793198]\n",
      "[-0.76844432 -1.25434848 -0.89817861 -0.4872649  -0.51174063 -0.72061496\n",
      " -1.24559929  1.17013796 -1.21091203  0.43806427 -3.136237    0.63153871\n",
      "  0.82086549  0.93091113  0.92053555  0.17438994  0.66933869 -1.92182553\n",
      " -1.40150304  0.88426581 -0.57543769  1.2165533  -0.03329479 -0.20943162\n",
      "  1.08044635 -0.06718248 -1.73916592  0.1352895   0.2537576  -1.69712476\n",
      " -1.0954704  -0.58285535  0.43185717 -0.27600702  0.73456132 -0.26018284\n",
      "  0.26639602 -2.07585853 -0.07781209  0.37752775 -0.10678269  1.10467615\n",
      "  1.42708224 -1.80037385  1.28201314 -0.89088326  0.25478092  1.05220623\n",
      "  2.42349904 -1.21595062]\n",
      "[ 0.72495709  0.14475979  0.40799764  0.27970063  0.2102936   0.275352\n",
      "  0.72461644 -0.16936523 -0.19879436 -0.2411809   0.70835163 -0.01461002\n",
      " -0.53636936  0.4026937  -0.28224234 -0.02006721 -0.50878279  0.07689739\n",
      "  0.46378586 -0.53857228  0.23905007 -0.27670408  0.59705694  0.23643145\n",
      "  0.15449354 -0.30468422  0.30732507 -0.02902881 -0.91378765  0.61498539\n",
      "  0.26272278 -0.23255971 -0.3643824   0.12010869 -0.76939813  0.04313749\n",
      " -0.17627059 -0.954228   -0.00793967  0.26889068  0.03144133 -0.35512265\n",
      "  1.3081617   0.08485011 -0.41405977 -0.52281661  0.0487735   0.10062892\n",
      " -0.82999856  0.00857342]\n",
      "[ 0.1862343  -0.11928814  0.64157575  0.18916738 -0.06164319  0.47478056\n",
      "  0.47767201 -0.18430513 -0.33668927 -0.22272462  0.23875412 -0.06915736\n",
      " -0.23535693  0.29813641  0.02456747  0.14366317 -0.279819   -0.29810535\n",
      "  0.40442709 -0.06078638 -0.10547512 -0.28741835  0.13661381  0.54007948\n",
      "  0.13600225 -0.42526897  0.21995522  0.06299158 -1.04646038  0.14730518\n",
      "  0.11352069 -0.24965977 -0.25518571  0.18733943 -0.47796998 -0.38788114\n",
      "  0.10482437 -0.65152299 -0.34698002  0.19844831  0.21128888 -0.21353695\n",
      "  0.88448856 -0.14694666 -0.27908491 -0.21768364  0.1904046   0.44926837\n",
      " -0.21636985 -0.19828246]\n",
      "[ 1.05550857 -0.27926652  0.98400598  0.08045572 -0.05270257  0.41348022\n",
      "  1.01086682 -0.37046194  0.07002196 -0.15861953  0.81261122 -0.09323271\n",
      " -0.76286181  0.16544116 -0.26127282  0.03361953 -0.6959076   0.17517962\n",
      "  0.48729948 -0.34830074  0.22367952 -0.25308473  0.84309748  0.40807068\n",
      " -0.1203797  -0.68781496  0.23007076 -0.06614775 -1.16505112  0.51602947\n",
      "  0.5325158  -0.46409493 -0.39054003  0.69429896 -0.88099836  0.05661839\n",
      "  0.01793963 -0.60253454 -0.51368322  0.23450647 -0.15243675 -0.00393843\n",
      "  0.91365248 -0.12235944 -0.24606708 -0.4594543  -0.02486733  0.27603408\n",
      " -1.02090076  0.18005401]\n",
      "[ 0.53154852  0.05822137  0.71695471 -0.09933173 -0.15213348  0.54945457\n",
      "  0.46212665 -0.09626168 -0.45852293  0.27409792  0.54554463  0.05781892\n",
      " -0.49956725 -0.09202479 -0.37892304 -0.77950287 -0.7555575  -0.09641011\n",
      "  0.32549681 -0.58387138  0.04535827 -0.27303458  0.33067667  0.54262813\n",
      " -0.26110005 -0.32205161  0.25482536 -0.21251364 -0.97184962  0.35363962\n",
      "  0.50528119 -0.36173027 -0.18802746  0.51400772 -0.58325209  0.16214805\n",
      "  0.04007741 -0.99699542  0.20381069  0.62732208 -0.17845907 -0.22152632\n",
      "  0.97577877  0.15832591 -0.22842396 -0.613457   -0.039155    0.20379021\n",
      " -0.25717802  0.12913693]\n",
      "[ 1.47608547e+00  6.16046426e-01 -6.64859964e-02  1.98194706e-02\n",
      "  1.90799499e-01  3.59283688e-02  9.38693759e-01  3.04836202e-02\n",
      " -4.05776165e-01 -3.29153196e-01  4.53885476e-01  2.33298877e-01\n",
      " -6.51445220e-01  5.74452951e-01 -5.75660094e-01 -7.75072161e-02\n",
      " -7.46323367e-01  9.62636480e-01  1.51235484e-01 -1.17508608e+00\n",
      "  8.80390472e-01  2.49045011e-01  1.47801163e+00 -7.88344344e-01\n",
      "  1.14371974e-03 -1.66303088e-01  1.61641463e-01 -1.61624573e-01\n",
      " -1.30788483e-01  7.84475515e-01  1.43908924e-01 -1.91293917e-01\n",
      " -8.18643529e-01  4.01192031e-01 -8.62952997e-01  4.79156167e-01\n",
      " -5.76226448e-01 -1.15930233e+00  3.55280306e-01 -2.42530886e-01\n",
      " -1.21595208e-01 -9.39705379e-01  1.33975134e+00 -2.95041531e-01\n",
      " -6.31825521e-01 -1.42370130e+00 -2.83156721e-01  2.85868340e-01\n",
      " -1.20384908e+00  4.29519704e-01]\n",
      "[ 0.65911129 -0.56532524  1.18898244  0.0909282  -0.18695495  0.83740123\n",
      "  0.90987382 -0.3578946  -0.0434072  -0.07577782  0.47337072 -0.11474554\n",
      " -0.40097285 -0.23743761  0.03351181 -0.01151638 -0.68860301 -0.08781029\n",
      "  0.52730607  0.09174726  0.13695406 -0.3738483   0.50086644  0.61459489\n",
      " -0.36579022 -0.89880208  0.2247167  -0.0375501  -1.37450747  0.1585492\n",
      "  0.57605422 -0.6207138  -0.3395134   0.6049483  -0.63571466 -0.15792889\n",
      "  0.31636853 -0.29490854 -0.59421177  0.16201423  0.05088653  0.15823958\n",
      "  0.70944428 -0.38412971 -0.02564572 -0.1435398   0.12045884  0.60281539\n",
      " -0.57773778 -0.05969448]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Retrieve a random sample of documents for a given cluster\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so the children see the advertise and then the parents have two possibilities\n",
      "-------------\n",
      "they learn this important things indirect so they donâ´t realise it\n",
      "-------------\n",
      "no wonder that kids are being spoilt nowadays because their parents want them to have a good childhood and buy their children everything they wish for\n",
      "-------------\n",
      "when the children see those ads they make their parents buy them these toys and are willing to watch even more tv\n",
      "-------------\n",
      "if they then get the plane and realise that the plane cant fly they will be very disappointed\n",
      "-------------\n",
      "already they do not understand that their parents work for buying the plays and that the plays cost money\n",
      "-------------\n",
      "the kids will see something that they like on television and they will say to their parents that they like to have this thing for christmas\n",
      "-------------\n",
      "adverts aret usefull for young children because they cant even buy things by themself\n",
      "-------------\n",
      "also it could be a good way to teach the kids that they can never have everything they want\n",
      "-------------\n",
      "children dont really know if theyre in need of the things they see in tv and they start crying if they see something they want\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "for i,t in enumerate(df_clusters.query(f\"cluster == {0}\").sample(10).iterrows()):\n",
    "    print(t[1][\"text\"])\n",
    "    print(\"-------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "data": {
      "text/plain": "(38715, 3)"
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clusters.shape\n",
    "\n",
    "# df_clusters.tokens[0]\n",
    "# df_clusters.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Most representative clusters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in my opinion the supporters of the ban have a point but they focus their concerns on the wrong medium since tvs have lost in importance as seen in one south park episode where cartman becomes a internet star and there is a christmas special to do something and one of the kids stars a petition to save the televison since thats what brought families together the best\n",
      "-------------\n",
      "so the question that posses itself is if it is okay to have global corporations subsequently implanting our desires in our brain and where do you draw the line between advertisments and brain control if you take coca cola for example a lot of studies have been made to show that it can cause serious health issues when consumed every day as well as when you consume too much of it for example bad teeth because of the sugar and other troubles you get for example stomach ulcers due to all the acids it contents\n",
      "-------------\n",
      "reproduction has been essential for the survival of every living being and by adapting and adding this special stage of our life to the laws of economic we not only create a bigger benefit for the market as whole but we also support families and toddlers where there is a need for knowledge or help for example if the kid suffers from minor medical issues and the parents are not capable of finding a solution\n",
      "-------------\n",
      "until this it seems to be fairly normal but what what follows is truly malicious the whole time the spot was set in motion there has been some kind of ultrahigh frequency tone which was not hearable for the human ear but unfortunately it affected our brain and made us want to purchase that product\n",
      "-------------\n",
      "i mean just look back at the old malboro commercials the malboro man was one of the coolest guys a lot of adults can remember nowadays and people start to conect beeing a cool person to smoking even if you probably know how bad it is for you and your body\n",
      "-------------\n",
      "a child whoâ´s watching aggresive movies can also be the most relaxed kid in the universe because it lets his dark side in the television so he can be fully relaxed in normal life\n",
      "-------------\n",
      "so for example television advertising focused on little children mostly is based on saying the name of the product many times with a cool voice in order to bring it into the viewers mind\n",
      "-------------\n",
      "in this case it is not a common tv but through the digital century you can use other electronical devices like mobile phones tablets or comnputer\n",
      "-------------\n",
      "in my opinion if there is a kid watching television with this age no one has to pay attention or has to change anything because in the end you do not know which series this child is skipping through so you will never have a hundert percent control over the different kids ad the lessons and experiences they learn and make through the television\n",
      "-------------\n",
      "there is no morning programm for children without advertisements of the must have dora the explorer writing book and matching to that the new backpack that will surely make you the coolest kid in school\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "test_cluster = 5\n",
    "most_representative_docs = np.argsort(\n",
    "    np.linalg.norm(vectorized_docs - clustering.cluster_centers_[test_cluster], axis=1)\n",
    ")\n",
    "# print(most_representative_docs[0])\n",
    "for d in most_representative_docs[:10]:\n",
    "    print(sentences[d])\n",
    "    print(\"-------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38715\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([1])"
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#array = clustering.cluster_centers_[0]\n",
    "print(len(vectorized_docs))\n",
    "#print(array)\n",
    "array = vectorized_docs[120].reshape(1,-1)\n",
    "convertedArray = array.astype(float)\n",
    "\n",
    "clustering.predict(convertedArray)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.41131788 -0.11716345  0.28971842  0.5577258  -0.5772017   0.9901962\n",
      "   0.96931005 -0.5613676  -0.3226952  -0.4045963   0.18726283  0.44057044\n",
      "  -0.42001647 -0.4549887  -0.8095281   0.07320678 -0.20656103 -0.8354379\n",
      "   0.20628421 -0.6952043  -0.29465434 -0.11770947 -0.03002735  0.10196522\n",
      "   0.37290215 -0.10642844  0.10388153 -0.5029228  -0.48719856  0.58507514\n",
      "  -0.10985485 -0.43433896 -0.43355358 -0.5003478  -0.255086    1.0371667\n",
      "  -0.56815755 -1.0393796  -0.58956033  0.4535848  -0.05363839 -0.7777182\n",
      "   1.1643625  -0.07477699 -0.07372791 -0.9933421  -0.22898763 -0.10050163\n",
      "  -0.13402921 -0.31821993]]\n",
      "[[ 0.41131788 -0.11716345  0.28971842  0.55772579 -0.57720172  0.99019623\n",
      "   0.96931005 -0.56136757 -0.3226952  -0.4045963   0.18726283  0.44057044\n",
      "  -0.42001647 -0.45498869 -0.80952811  0.07320678 -0.20656103 -0.83543789\n",
      "   0.20628421 -0.69520432 -0.29465434 -0.11770947 -0.03002735  0.10196522\n",
      "   0.37290215 -0.10642844  0.10388153 -0.50292277 -0.48719856  0.58507514\n",
      "  -0.10985485 -0.43433896 -0.43355358 -0.50034779 -0.255086    1.03716671\n",
      "  -0.56815755 -1.0393796  -0.58956033  0.45358479 -0.05363839 -0.77771819\n",
      "   1.16436255 -0.07477699 -0.07372791 -0.9933421  -0.22898763 -0.10050163\n",
      "  -0.13402921 -0.31821993]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\AppData\\Local\\Temp\\ipykernel_25384\\3934604238.py:8: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  convertedArray = array.astype(np.float)\n"
     ]
    }
   ],
   "source": [
    "    #print(array.reshape(1,-1))\n",
    "\n",
    "print(array)\n",
    "\n",
    "print(convertedArray)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n"
     ]
    }
   ],
   "source": [
    "## testing\n",
    "def vectorizeSentenceTest(sentences):\n",
    "    tokensSentenceslist = []\n",
    "    for s  in sentences:\n",
    "        wordsList = word_tokenize(s)\n",
    "        tokensSentenceslist.append(wordsList)\n",
    "    return tokensSentenceslist\n",
    "\n",
    "testTokens = vectorizeSentenceTest([\"In my opinion, supporters of the ban have a position, and they are looking at the wrong medium, as seen in a South Park episode where Cartman becomes an internet sensation and there is a Christmas special to do something, and one of the kids stars a petition to save the tv news because it is the best way to bring families together.\"])\n",
    "\n",
    "vectorized_docs_tesing = vectorize(testTokens, model=model, strategy=\"average\")\n",
    "\n",
    "def predictTest(vectorizedDocsTest):\n",
    "    array = vectorizedDocsTest\n",
    "    print(clustering.predict(array))\n",
    "    return\n",
    "#len(vectorized_docs_tesing), len(vectorized_docs_tesing[0])\n",
    "#print(vectorized_docs_tesing)\n",
    "\n",
    "predictTest(vectorized_docs_tesing)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}