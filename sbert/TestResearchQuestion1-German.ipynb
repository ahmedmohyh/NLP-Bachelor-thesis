{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Tests for the 2nd Research question (Culture)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk import tokenize\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "import re\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device=\"cuda\")\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "\n",
    "listStructureWords =  []\n",
    "\n",
    "dfSrtucturingWords = pd.read_excel(\"../structureWords.xlsx\")\n",
    "\n",
    "listStructureWords = dfSrtucturingWords.iloc[:,0].tolist()\n",
    "\n",
    "regex = r\"\\b(?:{})\\b\".format(\"|\".join(listStructureWords))\n",
    "sentences = []\n",
    "\n",
    "df_fileName = pd.DataFrame({})\n",
    "\n",
    "for filename in os.listdir(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\only100-German-teachers\"):\n",
    "   with open(os.path.join(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\only100-German-teachers\", filename)) as f:\n",
    "       text = f.read()\n",
    "       text = text.replace(\"ï»¿\",\"\")\n",
    "       sents = re.split(regex, text)\n",
    "       sents = tokenize.sent_tokenize(text)\n",
    "       for s in sents:\n",
    "           sentses = tokenize.sent_tokenize(s)\n",
    "           if (s.isspace() or len(s) ==0):\n",
    "               continue\n",
    "           s = s.lower()\n",
    "           if (len(s.split()) <5):\n",
    "               continue\n",
    "           for ss in sentses:\n",
    "                 if (len(ss.split()) >5):\n",
    "                     sentences.append(ss)\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "corpus_embeddings = model.encode(sentences, show_progress_bar =True, device=\"cuda\")\n",
    "\n",
    "def mbkmeans_clusters(X, k, mb=500, print_silhouette_values=False):\n",
    "    \"\"\"Generate clusters.\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches. Defaults to 500.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "\n",
    "k = 19\n",
    "\n",
    "clustering, cluster_labels = mbkmeans_clusters(X=corpus_embeddings, k=k, print_silhouette_values=True)\n",
    "\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"text\": sentences,\n",
    "    \"cluster\": cluster_labels\n",
    "})\n",
    "\n",
    "df_clusters.to_excel(\"GermanExcelResearchQ1.xlsx\")\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "\n",
    "df_test = pd.DataFrame({})\n",
    "\n",
    "df_ref = pd.read_excel(r\"D:\\USERS-Load\\PycharmProjects\\pythonProject\\CVS Files\\Teachers prompt\\Goldstandards.xlsx\")\n",
    "\n",
    "def_clusters = pd.read_excel(r\"GermanExcelResearchQ1.xlsx\")\n",
    "\n",
    "listRef = df_ref['Gold standard']\n",
    "\n",
    "listClusters = def_clusters['cluster']\n",
    "listNewSentences = def_clusters['text']\n",
    "listembReferences = []\n",
    "\n",
    "for ref in listRef:\n",
    "    listembReferences.append(model.encode(ref))\n",
    "\n",
    "cosListRefToClusters = []\n",
    "mm=0\n",
    "for emb in corpus_embeddings:\n",
    "    listMax = []\n",
    "    for embref in listembReferences:\n",
    "        cos_sim = util.cos_sim(emb, embref)\n",
    "        listMax.append(cos_sim)\n",
    "    index_max = np.argmax(listMax) #this is the right reference cluster\n",
    "    cosListRefToClusters.append(index_max)\n",
    "    df_test = df_test.append({\n",
    "        \"ID\" : mm,\n",
    "        \"text\": listNewSentences[mm],\n",
    "        \"cluster\": listClusters[mm],\n",
    "        \"cos Sim\": index_max\n",
    "              },ignore_index=True)\n",
    "    mm = mm+1\n",
    "\n",
    "\n",
    "df_test.to_excel(\"GermanExcelResearchQ1.xlsx\");\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "tpCount= 0\n",
    "tnCount  = 0\n",
    "\n",
    "fnCount = 0\n",
    "fpcount = 0\n",
    "\n",
    "\n",
    "dfPairEvaluation = pd.read_excel(r\"GermanExcelResearchQ1.xlsx\")\n",
    "\n",
    "iteration = dfPairEvaluation.count()['ID']\n",
    "\n",
    "for i in range(0,iteration):\n",
    "    for j in range (i+1, iteration):\n",
    "\n",
    "\n",
    "        clusterSystem1 = dfPairEvaluation.query(f\"ID == {i}\")['cluster'].iloc[0]\n",
    "        clusterGold1 = dfPairEvaluation.query(f\"ID == {i}\")['cos Sim'].iloc[0]\n",
    "        clusterSystem2 = dfPairEvaluation.query(f\"ID == {j}\")['cluster'].iloc[0]\n",
    "        clusterGold2 = dfPairEvaluation.query(f\"ID == {j}\")['cos Sim'].iloc[0]\n",
    "\n",
    "\n",
    "        if (clusterSystem1 == clusterSystem2):\n",
    "\n",
    "            if (clusterGold1 ==clusterGold2): # it is a true positive\n",
    "                tpCount = tpCount+ 1\n",
    "\n",
    "            else:\n",
    "                fnCount = fnCount + 1   # it is a false negative\n",
    "        else:\n",
    "            if (clusterGold1 !=clusterGold2):\n",
    "                tnCount = tnCount + 1  # it is a true negative\n",
    "            else:\n",
    "                fpcount = fpcount+1   # it is a false positive.\n",
    "\n",
    "print(\"the value of tp is\", tpCount)\n",
    "print(\"the value of tn is\", tnCount)\n",
    "print(\"the value of fn is\", fnCount)\n",
    "print(\"the value of fb is\", fpcount)\n",
    "\n",
    "accuracy = (tpCount + tnCount)/(tpCount +tnCount + fpcount+ fnCount)\n",
    "\n",
    "print(\"The accuracy is \",accuracy*100)  # the value is higher than the other accuracy.\n",
    "\n",
    "percision = tpCount / (tpCount + fpcount)\n",
    "\n",
    "print(\"The percision is \",percision*100)\n",
    "\n",
    "recall = tpCount / (tpCount + fnCount)\n",
    "\n",
    "print(\"The recall is \",recall*100)\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "from collections import defaultdict\n",
    "dfPurity = pd.read_excel(r\"GermanExcelResearchQ1.xlsx\")\n",
    "d = defaultdict(list)\n",
    "\n",
    "for j in range(0, k-1):\n",
    "    mylist = dfPurity.query(f\"cluster == {j}\")['cos Sim']\n",
    "    d[j].extend(mylist)\n",
    "\n",
    "rightCounter = 0\n",
    "\n",
    "for key, value in d.items():\n",
    "    mostFreq = max(set(value), key = value.count)\n",
    "    rightCounter = rightCounter + value.count(mostFreq)\n",
    "    #print(key , value.count(mostFreq))\n",
    "\n",
    "total = dfPurity.count()['ID']\n",
    "\n",
    "print(\"purity = \", (rightCounter/total)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk import tokenize\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "import re\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device=\"cuda\")\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "\n",
    "listStructureWords =  []\n",
    "\n",
    "dfSrtucturingWords = pd.read_excel(\"../structureWords.xlsx\")\n",
    "\n",
    "listStructureWords = dfSrtucturingWords.iloc[:,0].tolist()\n",
    "\n",
    "regex = r\"\\b(?:{})\\b\".format(\"|\".join(listStructureWords))\n",
    "sentences = []\n",
    "\n",
    "df_fileName = pd.DataFrame({})\n",
    "\n",
    "for filename in os.listdir(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\Only 100 - German Ads\"):\n",
    "   if (len(sentences) >= 1000):\n",
    "        break\n",
    "   with open(os.path.join(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\Only 100 - German Ads\", filename)) as f:\n",
    "       text = f.read()\n",
    "       text = text.replace(\"ï»¿\",\"\")\n",
    "       sents = re.split(regex, text)\n",
    "       sents = tokenize.sent_tokenize(text)\n",
    "       for s in sents:\n",
    "           sentses = tokenize.sent_tokenize(s)\n",
    "           if (s.isspace() or len(s) ==0):\n",
    "               continue\n",
    "           s = s.lower()\n",
    "           if (len(s.split()) <5):\n",
    "               continue\n",
    "           for ss in sentses:\n",
    "                 if (len(ss.split()) >5):\n",
    "                    sentences.append(ss)\n",
    "##############################################################################################################\n",
    "\n",
    "corpus_embeddings = model.encode(sentences, show_progress_bar =True, device=\"cuda\")\n",
    "\n",
    "def mbkmeans_clusters(X, k, mb=500, print_silhouette_values=False):\n",
    "    \"\"\"Generate clusters.\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches. Defaults to 500.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "\n",
    "k = 19\n",
    "\n",
    "clustering, cluster_labels = mbkmeans_clusters(X=corpus_embeddings, k=k, print_silhouette_values=True)\n",
    "\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"text\": sentences,\n",
    "    \"cluster\": cluster_labels\n",
    "})\n",
    "\n",
    "df_clusters.to_excel(\"GermanExcelResearchQ1-Normalised-AD.xlsx\")\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "\n",
    "df_test = pd.DataFrame({})\n",
    "\n",
    "df_ref = pd.read_excel(r\"D:\\USERS-Load\\PycharmProjects\\pythonProject\\CVS Files\\Teachers prompt\\Goldstandards.xlsx\")\n",
    "\n",
    "def_clusters = pd.read_excel(r\"GermanExcelResearchQ1-Normalised-AD.xlsx\")\n",
    "\n",
    "listRef = df_ref['Gold standard']\n",
    "\n",
    "listClusters = def_clusters['cluster']\n",
    "listNewSentences = def_clusters['text']\n",
    "listembReferences = []\n",
    "\n",
    "for ref in listRef:\n",
    "    listembReferences.append(model.encode(ref))\n",
    "\n",
    "cosListRefToClusters = []\n",
    "mm=0\n",
    "for emb in corpus_embeddings:\n",
    "    listMax = []\n",
    "    for embref in listembReferences:\n",
    "        cos_sim = util.cos_sim(emb, embref)\n",
    "        listMax.append(cos_sim)\n",
    "    index_max = np.argmax(listMax) #this is the right reference cluster\n",
    "    cosListRefToClusters.append(index_max)\n",
    "    df_test = df_test.append({\n",
    "        \"ID\" : mm,\n",
    "        \"text\": listNewSentences[mm],\n",
    "        \"cluster\": listClusters[mm],\n",
    "        \"cos Sim\": index_max\n",
    "              },ignore_index=True)\n",
    "    mm = mm+1\n",
    "\n",
    "df_test.to_excel(\"GermanExcelResearchQ1-Normalised-AD.xlsx\");\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test for the cosin Similarity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device=\"cuda\")\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "emb1 = model.encode(\"This is a red cat with a hat.\")\n",
    "emb2 = model.encode(\"My cat is red and has a hat\")\n",
    "\n",
    "cos_sim = util.cos_sim(emb1, emb2)\n",
    "print(\"Cosine-Similarity:\", cos_sim)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### final results cluster AD"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1173 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6b68f07fc6c45bba064775338541df8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 19\n",
      "Silhouette coefficient: 0.03\n",
      "Inertia:19603.66796875\n",
      "Silhouette values:\n",
      "    Cluster 17: Size:325 | Avg:0.59 | Min:0.09 | Max: 0.61\n",
      "    Cluster 4: Size:336 | Avg:0.47 | Min:0.02 | Max: 0.54\n",
      "    Cluster 8: Size:528 | Avg:0.20 | Min:-0.04 | Max: 0.34\n",
      "    Cluster 16: Size:2159 | Avg:0.14 | Min:0.05 | Max: 0.25\n",
      "    Cluster 3: Size:3914 | Avg:0.04 | Min:-0.12 | Max: 0.14\n",
      "    Cluster 10: Size:2032 | Avg:0.03 | Min:-0.14 | Max: 0.15\n",
      "    Cluster 5: Size:2025 | Avg:0.02 | Min:-0.11 | Max: 0.12\n",
      "    Cluster 9: Size:2693 | Avg:0.01 | Min:-0.17 | Max: 0.10\n",
      "    Cluster 18: Size:2007 | Avg:0.01 | Min:-0.07 | Max: 0.10\n",
      "    Cluster 11: Size:2191 | Avg:0.01 | Min:-0.14 | Max: 0.11\n",
      "    Cluster 13: Size:3146 | Avg:0.01 | Min:-0.10 | Max: 0.09\n",
      "    Cluster 2: Size:2561 | Avg:0.01 | Min:-0.08 | Max: 0.12\n",
      "    Cluster 12: Size:1747 | Avg:0.01 | Min:-0.14 | Max: 0.12\n",
      "    Cluster 0: Size:3182 | Avg:0.01 | Min:-0.09 | Max: 0.09\n",
      "    Cluster 6: Size:1077 | Avg:0.00 | Min:-0.14 | Max: 0.09\n",
      "    Cluster 15: Size:1545 | Avg:0.00 | Min:-0.11 | Max: 0.10\n",
      "    Cluster 7: Size:1481 | Avg:-0.02 | Min:-0.16 | Max: 0.09\n",
      "    Cluster 14: Size:2861 | Avg:-0.03 | Min:-0.17 | Max: 0.04\n",
      "    Cluster 1: Size:1695 | Avg:-0.06 | Min:-0.17 | Max: 0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:43: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  result = getattr(asarray(obj), method)(*args, **kwds)\n",
      "C:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = getattr(asarray(obj), method)(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import string\n",
    "from nltk import tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "import sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "from sklearn.cluster import KMeans\n",
    "import re\n",
    "import statistics\n",
    "from keras.layers import Average\n",
    "\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device=\"cuda\")\n",
    "\n",
    "listStructureWords =  []\n",
    "dfSrtucturingWords = pd.read_excel(\"../structureWords.xlsx\")\n",
    "listStructureWords = dfSrtucturingWords.iloc[:,0].tolist()\n",
    "regex = r\"\\b(?:{})\\b\".format(\"|\".join(listStructureWords))\n",
    "\n",
    "sentences = []\n",
    "fileNames= []\n",
    "j=0\n",
    "\n",
    "\n",
    "for filename in os.listdir(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\TelevisonMergedT1+T2\"):\n",
    "   my_file = Path(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\TelevisonMergedT1+T2\", filename)\n",
    "   if my_file.is_file():\n",
    "       text = my_file.read_text(encoding='utf-8-sig')\n",
    "       text = text.replace(\"ï»¿\",\"\")\n",
    "       sents = re.split(regex, text)\n",
    "       sents = tokenize.sent_tokenize(text)\n",
    "       for s in sents:\n",
    "           sentses = tokenize.sent_tokenize(s)\n",
    "           if (s.isspace() or len(s) ==0):\n",
    "               continue\n",
    "           s = s.lower()\n",
    "           if (len(s.split()) <5):\n",
    "               continue\n",
    "           for ss in sentses:\n",
    "                 if (len(ss.split()) >5):\n",
    "                     sentences.append(ss)\n",
    "                     fileNames.append(filename)\n",
    "\n",
    "corpus_embeddings = model.encode(sentences, show_progress_bar =True, device=\"cuda\")\n",
    "\n",
    "\n",
    "def mbkmeans_clusters(X, k, mb=500, print_silhouette_values=False):\n",
    "    \"\"\"Generate clusters.\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches. Defaults to 500.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_\n",
    "\n",
    "k = 19\n",
    "clustering, cluster_labels = mbkmeans_clusters(X=corpus_embeddings, k=k, print_silhouette_values=True)\n",
    "\n",
    "\n",
    "df_ref = pd.read_excel(r\"D:\\USERS-Load\\PycharmProjects\\pythonProject\\CVS Files\\Ads prompt\\01.08.22\\new Gold standards.xlsx\")\n",
    "\n",
    "listRef = df_ref['Gold standard']\n",
    "\n",
    "listembReferences = []\n",
    "\n",
    "for ref in listRef:\n",
    "    listembReferences.append(model.encode(ref))\n",
    "\n",
    "listIndexMax =[]\n",
    "\n",
    "for emb in corpus_embeddings:\n",
    "    listMax = []\n",
    "    for embref in listembReferences:\n",
    "        cos_sim = util.cos_sim(emb, embref)\n",
    "        listMax.append(cos_sim)\n",
    "    index_max = np.argmax(listMax) #this is the right reference cluster\n",
    "    listIndexMax.append(index_max)\n",
    "\n",
    "\n",
    "df_final = pd.DataFrame({\n",
    "    \"sentence\": sentences,\n",
    "    \"fileName\": fileNames,\n",
    "    \"cluster\": cluster_labels,\n",
    "    \"FinalCluster\": listIndexMax\n",
    "})\n",
    "\n",
    "df_final.to_excel(\"FinalAdResults.xlsx\");\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}