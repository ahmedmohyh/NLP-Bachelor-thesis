{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Testing the quality of the argument references."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from pytorch checkpoint file for 'C:\\Users\\Ahmed/.cache\\torch\\sentence_transformers\\sentence-transformers_all-mpnet-base-v2\\pytorch_model.bin' at 'C:\\Users\\Ahmed/.cache\\torch\\sentence_transformers\\sentence-transformers_all-mpnet-base-v2\\pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\modeling_utils.py:461\u001B[0m, in \u001B[0;36mload_state_dict\u001B[1;34m(checkpoint_file)\u001B[0m\n\u001B[0;32m    460\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 461\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py:592\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001B[0m\n\u001B[0;32m    591\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mload(opened_file)\n\u001B[1;32m--> 592\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _load(opened_zipfile, map_location, pickle_module, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpickle_load_args)\n\u001B[0;32m    593\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpickle_load_args)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py:851\u001B[0m, in \u001B[0;36m_load\u001B[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001B[0m\n\u001B[0;32m    850\u001B[0m unpickler\u001B[38;5;241m.\u001B[39mpersistent_load \u001B[38;5;241m=\u001B[39m persistent_load\n\u001B[1;32m--> 851\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43munpickler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    853\u001B[0m torch\u001B[38;5;241m.\u001B[39m_utils\u001B[38;5;241m.\u001B[39m_validate_loaded_sparse_tensors()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_utils.py:138\u001B[0m, in \u001B[0;36m_rebuild_tensor_v2\u001B[1;34m(storage, storage_offset, size, stride, requires_grad, backward_hooks)\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_rebuild_tensor_v2\u001B[39m(storage, storage_offset, size, stride, requires_grad, backward_hooks):\n\u001B[1;32m--> 138\u001B[0m     tensor \u001B[38;5;241m=\u001B[39m \u001B[43m_rebuild_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_offset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    139\u001B[0m     tensor\u001B[38;5;241m.\u001B[39mrequires_grad \u001B[38;5;241m=\u001B[39m requires_grad\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_utils.py:133\u001B[0m, in \u001B[0;36m_rebuild_tensor\u001B[1;34m(storage, storage_offset, size, stride)\u001B[0m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_rebuild_tensor\u001B[39m(storage, storage_offset, size, stride):\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;66;03m# first construct a tensor with the correct dtype/device\u001B[39;00m\n\u001B[1;32m--> 133\u001B[0m     t \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m([], dtype\u001B[38;5;241m=\u001B[39mstorage\u001B[38;5;241m.\u001B[39mdtype, device\u001B[38;5;241m=\u001B[39mstorage\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    134\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mset_(storage, storage_offset, size, stride)\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'torch' has no attribute 'tensor'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\modeling_utils.py:465\u001B[0m, in \u001B[0;36mload_state_dict\u001B[1;34m(checkpoint_file)\u001B[0m\n\u001B[0;32m    464\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(checkpoint_file) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m--> 465\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mversion\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    466\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[0;32m    467\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou seem to have cloned a repository without having git-lfs installed. Please install \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    468\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgit-lfs and run `git lfs install` followed by `git lfs pull` in the folder \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    469\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou cloned.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    470\u001B[0m         )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\encodings\\cp1252.py:23\u001B[0m, in \u001B[0;36mIncrementalDecoder.decode\u001B[1;34m(self, input, final)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m---> 23\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcodecs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcharmap_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdecoding_table\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'charmap' codec can't decode byte 0x81 in position 1720: character maps to <undefined>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 15>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m##############################################################################################################\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mSentenceTransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mall-mpnet-base-v2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m##############################################################################################################\u001B[39;00m\n\u001B[0;32m     21\u001B[0m listStructureWords \u001B[38;5;241m=\u001B[39m  []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:95\u001B[0m, in \u001B[0;36mSentenceTransformer.__init__\u001B[1;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001B[0m\n\u001B[0;32m     87\u001B[0m         snapshot_download(model_name_or_path,\n\u001B[0;32m     88\u001B[0m                             cache_dir\u001B[38;5;241m=\u001B[39mcache_folder,\n\u001B[0;32m     89\u001B[0m                             library_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentence-transformers\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     90\u001B[0m                             library_version\u001B[38;5;241m=\u001B[39m__version__,\n\u001B[0;32m     91\u001B[0m                             ignore_files\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mflax_model.msgpack\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrust_model.ot\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtf_model.h5\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m     92\u001B[0m                             use_auth_token\u001B[38;5;241m=\u001B[39muse_auth_token)\n\u001B[0;32m     94\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(model_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodules.json\u001B[39m\u001B[38;5;124m'\u001B[39m)):    \u001B[38;5;66;03m#Load as SentenceTransformer model\u001B[39;00m\n\u001B[1;32m---> 95\u001B[0m     modules \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_sbert_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:   \u001B[38;5;66;03m#Load with AutoModel\u001B[39;00m\n\u001B[0;32m     97\u001B[0m     modules \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load_auto_model(model_path)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:840\u001B[0m, in \u001B[0;36mSentenceTransformer._load_sbert_model\u001B[1;34m(self, model_path)\u001B[0m\n\u001B[0;32m    838\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m module_config \u001B[38;5;129;01min\u001B[39;00m modules_config:\n\u001B[0;32m    839\u001B[0m     module_class \u001B[38;5;241m=\u001B[39m import_from_string(module_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m--> 840\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[43mmodule_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodule_config\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpath\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    841\u001B[0m     modules[module_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m]] \u001B[38;5;241m=\u001B[39m module\n\u001B[0;32m    843\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m modules\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:137\u001B[0m, in \u001B[0;36mTransformer.load\u001B[1;34m(input_path)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(sbert_config_path) \u001B[38;5;28;01mas\u001B[39;00m fIn:\n\u001B[0;32m    136\u001B[0m     config \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(fIn)\n\u001B[1;32m--> 137\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Transformer(model_name_or_path\u001B[38;5;241m=\u001B[39minput_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:29\u001B[0m, in \u001B[0;36mTransformer.__init__\u001B[1;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdo_lower_case \u001B[38;5;241m=\u001B[39m do_lower_case\n\u001B[0;32m     28\u001B[0m config \u001B[38;5;241m=\u001B[39m AutoConfig\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name_or_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_args, cache_dir\u001B[38;5;241m=\u001B[39mcache_dir)\n\u001B[1;32m---> 29\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(tokenizer_name_or_path \u001B[38;5;28;01mif\u001B[39;00m tokenizer_name_or_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m model_name_or_path, cache_dir\u001B[38;5;241m=\u001B[39mcache_dir, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtokenizer_args)\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m#No max_seq_length set. Try to infer from model\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:49\u001B[0m, in \u001B[0;36mTransformer._load_model\u001B[1;34m(self, model_name_or_path, config, cache_dir)\u001B[0m\n\u001B[0;32m     47\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load_t5_model(model_name_or_path, config, cache_dir)\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 49\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:446\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    444\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    445\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[1;32m--> 446\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    447\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    448\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    449\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    450\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\modeling_utils.py:2132\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   2129\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m from_pt:\n\u001B[0;32m   2130\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_sharded \u001B[38;5;129;01mand\u001B[39;00m state_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2131\u001B[0m         \u001B[38;5;66;03m# Time to load the checkpoint\u001B[39;00m\n\u001B[1;32m-> 2132\u001B[0m         state_dict \u001B[38;5;241m=\u001B[39m \u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresolved_archive_file\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2134\u001B[0m     \u001B[38;5;66;03m# set dtype to instantiate the model under:\u001B[39;00m\n\u001B[0;32m   2135\u001B[0m     \u001B[38;5;66;03m# 1. If torch_dtype is not None, we use that dtype\u001B[39;00m\n\u001B[0;32m   2136\u001B[0m     \u001B[38;5;66;03m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001B[39;00m\n\u001B[0;32m   2137\u001B[0m     \u001B[38;5;66;03m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001B[39;00m\n\u001B[0;32m   2138\u001B[0m     \u001B[38;5;66;03m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001B[39;00m\n\u001B[0;32m   2139\u001B[0m     dtype_orig \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\modeling_utils.py:477\u001B[0m, in \u001B[0;36mload_state_dict\u001B[1;34m(checkpoint_file)\u001B[0m\n\u001B[0;32m    472\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    473\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to locate the file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcheckpoint_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m which is necessary to load this pretrained \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    474\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel. Make sure you have saved the model properly.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    475\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    476\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mUnicodeDecodeError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m--> 477\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[0;32m    478\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to load weights from pytorch checkpoint file for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcheckpoint_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    479\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mat \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcheckpoint_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    480\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    481\u001B[0m     )\n",
      "\u001B[1;31mOSError\u001B[0m: Unable to load weights from pytorch checkpoint file for 'C:\\Users\\Ahmed/.cache\\torch\\sentence_transformers\\sentence-transformers_all-mpnet-base-v2\\pytorch_model.bin' at 'C:\\Users\\Ahmed/.cache\\torch\\sentence_transformers\\sentence-transformers_all-mpnet-base-v2\\pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk import tokenize\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "import re\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device=\"cuda\")\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "\n",
    "listStructureWords =  []\n",
    "\n",
    "dfSrtucturingWords = pd.read_excel(\"../structureWords.xlsx\")\n",
    "\n",
    "listStructureWords = dfSrtucturingWords.iloc[:,0].tolist()\n",
    "\n",
    "regex = r\"\\b(?:{})\\b\".format(\"|\".join(listStructureWords))\n",
    "sentences = []\n",
    "\n",
    "df_fileName = pd.DataFrame({})\n",
    "\n",
    "for filename in os.listdir(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\TeachersMerged T1+T2\"):\n",
    "   with open(os.path.join(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\TeachersMerged T1+T2\", filename)) as f:\n",
    "       text = f.read()\n",
    "       text = text.replace(\"ï»¿\",\"\")\n",
    "       sents = re.split(regex, text)\n",
    "       sents = tokenize.sent_tokenize(text)\n",
    "       for s in sents:\n",
    "           sentses = tokenize.sent_tokenize(s)\n",
    "           if (s.isspace() or len(s) ==0):\n",
    "               continue\n",
    "           s = s.lower()\n",
    "           if (len(s.split()) <5):\n",
    "               continue\n",
    "           for ss in sentses:\n",
    "                 if (len(ss.split()) >5):\n",
    "                     sentences.append(ss)\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "corpus_embeddings = model.encode(sentences, show_progress_bar =True, device=\"cuda\")\n",
    "\n",
    "def mbkmeans_clusters(X, k, mb=500, print_silhouette_values=False):\n",
    "    \"\"\"Generate clusters.\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches. Defaults to 500.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "\n",
    "k = 19\n",
    "\n",
    "clustering, cluster_labels = mbkmeans_clusters(X=corpus_embeddings, k=k, print_silhouette_values=True)\n",
    "\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"text\": sentences,\n",
    "    \"cluster\": cluster_labels\n",
    "})\n",
    "\n",
    "df_clusters.to_excel(\"TestClusterReference.xlsx\")\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "\n",
    "df_test = pd.DataFrame({})\n",
    "\n",
    "df_ref = pd.read_excel(r\"D:\\USERS-Load\\PycharmProjects\\pythonProject\\CVS Files\\Teachers prompt\\Goldstandards.xlsx\")\n",
    "\n",
    "def_clusters = pd.read_excel(r\"TestClusterReference.xlsx\")\n",
    "\n",
    "listRef = df_ref['Gold standard']\n",
    "\n",
    "listClusters = def_clusters['cluster']\n",
    "listNewSentences = def_clusters['text']\n",
    "listembReferences = []\n",
    "\n",
    "for ref in listRef:\n",
    "    listembReferences.append(model.encode(ref))\n",
    "\n",
    "cosListRefToClusters = []\n",
    "mm=0\n",
    "countMissed =0\n",
    "for emb in corpus_embeddings:\n",
    "    listMax = []\n",
    "    for embref in listembReferences:\n",
    "        cos_sim = util.cos_sim(emb, embref)\n",
    "        listMax.append(cos_sim)\n",
    "    index_max = np.argmax(listMax) #this is the right reference cluster\n",
    "    if (listMax[index_max] <0.5): #threshold to know if this item does not match any of the references!!!\n",
    "        countMissed = countMissed +1\n",
    "        df_test = df_test.append({\n",
    "        \"ID\" : mm,\n",
    "        \"text\": listNewSentences[mm],\n",
    "        \"cluster\": listClusters[mm],\n",
    "        \"cos Sim\": \"NOT ASSIGNED\"\n",
    "              },ignore_index=True)\n",
    "        continue\n",
    "\n",
    "    cosListRefToClusters.append(index_max)\n",
    "    df_test = df_test.append({\n",
    "        \"ID\" : mm,\n",
    "        \"text\": listNewSentences[mm],\n",
    "        \"cluster\": listClusters[mm],\n",
    "        \"cos Sim\": index_max\n",
    "              },ignore_index=True)\n",
    "    mm = mm+1\n",
    "\n",
    "\n",
    "df_test.to_excel(\"TestClusterReference.xlsx\");\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13065\n"
     ]
    }
   ],
   "source": [
    "print(countMissed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}