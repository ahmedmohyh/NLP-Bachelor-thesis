{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "b_list= [[1,2,3,4,5],[6,7,8,9,10]]\n",
    "z= np.array(b_list)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(z.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "arr = np.array([[1,2,3],[4,5,6]])\n",
    "arr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list = [1,2,3,4]\n",
    "\n",
    "arr=  np.array(list)\n",
    "\n",
    "arr.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "paragraphs = []\n",
    "\n",
    "for filename in os.listdir(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\TestingEssays\"):\n",
    "   with open(os.path.join(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\TestingEssays\", filename)) as f:\n",
    "       text = f.read()\n",
    "       #text = text.replace(\"ï»¿\",\"\")\n",
    "       #paragraphs = text.split('\\n\\n')\n",
    "       for line in text.split('\\n'):\n",
    "         print (\"This is \" + line + \" !\")\n",
    "       y = open(os.path.join(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\TestingEssays\", filename), 'r')\n",
    "       paragraph = 0\n",
    "       lines = y.readlines()\n",
    "       for idx, line in enumerate(lines):\n",
    "         if not line == '\\n':\n",
    "             m = re.search(r'\\w', line)\n",
    "             str = m.group(0)\n",
    "         try:\n",
    "            if line == '\\n' and str in lines[idx-1]:\n",
    "             paragraph +=1\n",
    "         except:\n",
    "             pass\n",
    "       if lines[-1] != '\\n': # if the last line is not a new line, count a paragraph.\n",
    "        paragraph +=1\n",
    "\n",
    "\n",
    "# for para in paragraphs:\n",
    "#     print(para)\n",
    "#     print(\"###########################\")\n",
    "\n",
    "print (paragraph)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "f = open('../test.txt', 'r')\n",
    "\n",
    "paragraph = 0\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "#print(lines)\n",
    "\n",
    "for idx, line in enumerate(lines):\n",
    "    print(line)\n",
    "    if not line == '\\n':\n",
    "        m = re.search(r'\\w', line)\n",
    "        str = m.group(0)\n",
    "\n",
    "    try:\n",
    "        # if the line is a newline, and the previous line has a str in it, then\n",
    "        # count it as a paragraph.\n",
    "        if line == '\\n' and str in lines[idx-1]:\n",
    "            paragraph +=1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if lines[-1] != '\\n': # if the last line is not a new line, count a paragraph.\n",
    "    paragraph +=1\n",
    "\n",
    "print (paragraph)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f = open('../test.txt', 'r')\n",
    "text = f.read()  #\\n\\n denotes there is a blank line in between paragraphs.\n",
    "\n",
    "text = text.split(\"\\n\\n\")\n",
    "\n",
    "for inp in text:\n",
    "    print(inp)\n",
    "    print(\"########################\")\n",
    "\n",
    "print(len(text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### structuring words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "listStructureWords =  []\n",
    "\n",
    "df = pd.read_excel(\"structureWords.xlsx\")\n",
    "listStructureWords = df.iloc[:,0].tolist()\n",
    "\n",
    "print(listStructureWords)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex = r\"\\b(?:{})\\b\".format(\"|\".join(listStructureWords))\n",
    "\n",
    "s = \"\"\"First of all,\n",
    "\n",
    "########### I would say #####\n",
    "\n",
    " that it would be difficult to stop televison advertising which is directed toward young children in the ages from two to five.\n",
    "\"Televison advertising could be helpful especialy for parents if they don´t have an idea concerning to give a present to their child.\n",
    "\"But\n",
    " #########   on the other hand   ###############\n",
    "\n",
    "\n",
    " , watching telivison advertising in these ages can lead to the missing ability to appreciate things which are advertised in the TV\n",
    "\n",
    "############## , such as #############\n",
    "\n",
    "  toys or electronic devises to play with. If they always want more toys, the parents maybe will follow their wishes to make their children happy and make these wishes come true, which can lead to the missing abilities mentioned before. They also wouldn´t know the value of these things.\n",
    "\"My opinion to this topic is, that televsion  advertising directed to young children should be stopped. Furthermore, the parents have to have an eye on their children if they watch TV. Watching TV in these ages can also be discussed, whether it´s good for them.\"\"\"\n",
    "\n",
    "res = [r.strip() for r in re.split(regex, s)]\n",
    "for s in res:\n",
    "    print(s)\n",
    "    print(\"#################\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### testing on all the data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk import tokenize\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-mpnet-base-v2', device=\"cuda\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "listStructureWords =  []\n",
    "\n",
    "dfSrtucturingWords = pd.read_excel(\"structureWords.xlsx\")\n",
    "\n",
    "listStructureWords = dfSrtucturingWords.iloc[:,0].tolist()\n",
    "\n",
    "regex = r\"\\b(?:{})\\b\".format(\"|\".join(listStructureWords))\n",
    "sentences = []\n",
    "\n",
    "df_fileName = pd.DataFrame({})\n",
    "\n",
    "for filename in os.listdir(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\10 essays for essays reference\\2\"):\n",
    "   with open(os.path.join(r\"D:\\UDE\\6th Semester\\MEMS\\MEWS Data\\MEWS_Essays\\MEWS_Essays\\Essays_all\\10 essays for essays reference\\2\", filename)) as f:\n",
    "       text = f.read()\n",
    "       text = text.replace(\"ï»¿\",\"\")\n",
    "       sents = re.split(regex, text)\n",
    "       sents = tokenize.sent_tokenize(text)\n",
    "       for s in sents:\n",
    "           sentses = tokenize.sent_tokenize(s)\n",
    "           if (s.isspace() or len(s) ==0):\n",
    "               continue\n",
    "           s = s.lower()\n",
    "           if (len(s.split()) <5):\n",
    "               continue\n",
    "           for ss in sentses:\n",
    "                 if (len(ss.split()) >5):\n",
    "                     sentences.append(ss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40234da851144d319ccaa176d661994d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_embeddings = model.encode(sentences, show_progress_bar =True, device=\"cuda\")\n",
    "\n",
    "def mbkmeans_clusters(X, k, mb=500, print_silhouette_values=False):\n",
    "    \"\"\"Generate clusters.\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches. Defaults to 500.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k = 19\n",
    "\n",
    "clustering, cluster_labels = mbkmeans_clusters(X=corpus_embeddings, k=k, print_silhouette_values=True)\n",
    "\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"text\": sentences,\n",
    "    \"cluster\": cluster_labels\n",
    "})\n",
    "\n",
    "df_clusters.to_excel(\"YourNewExcel.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame({})\n",
    "\n",
    "df_ref = pd.read_excel(r\"D:\\USERS-Load\\PycharmProjects\\pythonProject\\CVS Files\\01.08.22\\new Gold standards.xlsx\")\n",
    "\n",
    "def_clusters = pd.read_excel(r\"YourNewExcel.xlsx\")\n",
    "\n",
    "listRef = df_ref['Gold standard']\n",
    "\n",
    "listClusters = def_clusters['cluster']\n",
    "listNewSentences = def_clusters['text']\n",
    "listembReferences = []\n",
    "\n",
    "for ref in listRef:\n",
    "    listembReferences.append(model.encode(ref))\n",
    "\n",
    "cosListRefToClusters = []\n",
    "mm=0\n",
    "for emb in corpus_embeddings:\n",
    "    listMax = []\n",
    "    for embref in listembReferences:\n",
    "        cos_sim = util.cos_sim(emb, embref)\n",
    "        listMax.append(cos_sim)\n",
    "    index_max = np.argmax(listMax) #this is the right reference cluster\n",
    "    cosListRefToClusters.append(index_max)\n",
    "    df_test = df_test.append({\n",
    "        \"ID\" : mm,\n",
    "        \"text\": listNewSentences[mm],\n",
    "        \"cluster\": listClusters[mm],\n",
    "        \"cos Sim\": index_max\n",
    "              },ignore_index=True)\n",
    "    mm = mm+1\n",
    "\n",
    "\n",
    "df_test.to_excel(\"YourNewExcel.xlsx\");"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tpCount= 0\n",
    "tnCount  = 0\n",
    "\n",
    "fnCount = 0\n",
    "fpcount = 0\n",
    "\n",
    "\n",
    "dfPairEvaluation = pd.read_excel(r\"YourNewExcel.xlsx\")\n",
    "\n",
    "iteration = dfPairEvaluation.count()['ID']\n",
    "\n",
    "for i in range(0,iteration):\n",
    "    for j in range (i+1, iteration):\n",
    "\n",
    "\n",
    "        clusterSystem1 = dfPairEvaluation.query(f\"ID == {i}\")['cluster'].iloc[0]\n",
    "        clusterGold1 = dfPairEvaluation.query(f\"ID == {i}\")['cos Sim'].iloc[0]\n",
    "        clusterSystem2 = dfPairEvaluation.query(f\"ID == {j}\")['cluster'].iloc[0]\n",
    "        clusterGold2 = dfPairEvaluation.query(f\"ID == {j}\")['cos Sim'].iloc[0]\n",
    "\n",
    "\n",
    "        if (clusterSystem1 == clusterSystem2):\n",
    "\n",
    "            if (clusterGold1 ==clusterGold2): # it is a true positive\n",
    "                tpCount = tpCount+ 1\n",
    "\n",
    "            else:\n",
    "                fnCount = fnCount + 1   # it is a false negative\n",
    "        else:\n",
    "            if (clusterGold1 !=clusterGold2):\n",
    "                tnCount = tnCount + 1  # it is a true negative\n",
    "            else:\n",
    "                fpcount = fpcount+1   # it is a false positive.\n",
    "\n",
    "print(\"the value of tp is\", tpCount)\n",
    "print(\"the value of tn is\", tnCount)\n",
    "print(\"the value of fn is\", fnCount)\n",
    "print(\"the value of fb is\", fpcount)\n",
    "\n",
    "accuracy = (tpCount + tnCount)/(tpCount +tnCount + fpcount+ fnCount)\n",
    "\n",
    "print(\"The accuracy is \",accuracy*100)  # the value is higher than the other accuracy.\n",
    "\n",
    "percision = tpCount / (tpCount + fpcount)\n",
    "\n",
    "print(\"The percision is \",percision*100)\n",
    "\n",
    "recall = tpCount / (tpCount + fnCount)\n",
    "\n",
    "print(\"The recall is \",recall*100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "dfPurity = pd.read_excel(r\"YourNewExcel.xlsx\")\n",
    "d = defaultdict(list)\n",
    "\n",
    "for j in range(0, k-1):\n",
    "    mylist = dfPurity.query(f\"cluster == {j}\")['cos Sim']\n",
    "    d[j].extend(mylist)\n",
    "\n",
    "rightCounter = 0\n",
    "\n",
    "for key, value in d.items():\n",
    "    mostFreq = max(set(value), key = value.count)\n",
    "    rightCounter = rightCounter + value.count(mostFreq)\n",
    "    #print(key , value.count(mostFreq))\n",
    "\n",
    "total = dfPurity.count()['ID']\n",
    "\n",
    "print(\"purity = \", (rightCounter/total)*100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### calculate the count of each culture for the first research question"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_numbersAuto = pd.read_excel(r\"D:\\USERS-Load\\PycharmProjects\\pythonProject\\sbert\\GermanExcelResearchQ1-Normalised-AD.xlsx\")\n",
    "df_test = pd.DataFrame({})\n",
    "\n",
    "for j in range(0, 19):\n",
    "    listClustersIDRef = df_numbersAuto.query(f\"cosSim == {j}\")['ID']\n",
    "    df_test = df_test.append({\n",
    "        \"Cluster Reference\" : j,\n",
    "        \"count\": len(listClustersIDRef)\n",
    "              },ignore_index=True)\n",
    "\n",
    "df_test.to_excel(\"GermanExcelResearchQ1 - AD - Normalised-Counts.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### calculate the purity and the accuracy for the hand made 10 essays"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tpCount= 0\n",
    "tnCount  = 0\n",
    "\n",
    "fnCount = 0\n",
    "fpcount = 0\n",
    "\n",
    "\n",
    "dfPairEvaluation = pd.read_csv(r\"D:\\USERS-Load\\PycharmProjects\\pythonProject\\CSV Files\\Evaluation\\Gold vs Clusters - Word2Vec.csv\")\n",
    "\n",
    "iteration = dfPairEvaluation.count()['ID']\n",
    "\n",
    "for i in range(0,iteration):\n",
    "    for j in range (i+1, iteration):\n",
    "\n",
    "\n",
    "        clusterSystem1 = dfPairEvaluation.query(f\"ID == {i}\")['cluster'].iloc[0]\n",
    "        clusterGold1 = dfPairEvaluation.query(f\"ID == {i}\")['GOLDStandards'].iloc[0]\n",
    "        clusterSystem2 = dfPairEvaluation.query(f\"ID == {j}\")['cluster'].iloc[0]\n",
    "        clusterGold2 = dfPairEvaluation.query(f\"ID == {j}\")['GOLDStandards'].iloc[0]\n",
    "\n",
    "\n",
    "        if (clusterSystem1 == clusterSystem2):\n",
    "\n",
    "            if (clusterGold1 ==clusterGold2): # it is a true positive\n",
    "                tpCount = tpCount+ 1\n",
    "\n",
    "            else:\n",
    "                fnCount = fnCount + 1   # it is a false negative\n",
    "        else:\n",
    "            if (clusterGold1 !=clusterGold2):\n",
    "                tnCount = tnCount + 1  # it is a true negative\n",
    "            else:\n",
    "                fpcount = fpcount+1   # it is a false positive.\n",
    "\n",
    "print(\"the value of tp is\", tpCount)\n",
    "print(\"the value of tn is\", tnCount)\n",
    "print(\"the value of fn is\", fnCount)\n",
    "print(\"the value of fb is\", fpcount)\n",
    "\n",
    "accuracy = (tpCount + tnCount)/(tpCount +tnCount + fpcount+ fnCount)\n",
    "\n",
    "print(\"The accuracy is \",accuracy*100)  # the value is higher than the other accuracy.\n",
    "\n",
    "percision = tpCount / (tpCount + fpcount)\n",
    "\n",
    "print(\"The percision is \",percision*100)\n",
    "\n",
    "recall = tpCount / (tpCount + fnCount)\n",
    "\n",
    "print(\"The recall is \",recall*100)\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "from collections import defaultdict\n",
    "dfPurity = pd.read_csv(r\"D:\\USERS-Load\\PycharmProjects\\pythonProject\\CSV Files\\Evaluation\\Gold vs Clusters - Word2Vec.csv\")\n",
    "d = defaultdict(list)\n",
    "\n",
    "for j in range(0, 19):\n",
    "    if (j==3 or j==9):\n",
    "        continue\n",
    "    mylist = dfPurity.query(f\"cluster == {j}\")['GOLDStandards']\n",
    "    d[j].extend(mylist)\n",
    "\n",
    "rightCounter = 0\n",
    "\n",
    "for key, value in d.items():\n",
    "    mostFreq = max(set(value), key = value.count)\n",
    "    rightCounter = rightCounter + value.count(mostFreq)\n",
    "    #print(key , value.count(mostFreq))\n",
    "\n",
    "total = dfPurity.count()['ID']\n",
    "\n",
    "print(\"purity = \", (rightCounter/total)*100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/Ahmed/Downloads/archive/df_full_premierleague.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.drop(df.iloc[:, 37:114], inplace=True, axis=1)\n",
    "\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "df.to_csv(\"newCsv.csv\",index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test all the combinataion of vectorizer and the splitting"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from Utility.ThesisUtility import mbkmeans_clusters,writeClustersToExcel,getSentenceUsingStructringWords,printPairsEvaluation,printPurity,iterateAndAssignGoldStandard\n",
    "\n",
    "dataFilePath = \"../Dataset/Essays_all/10 essays for essays reference - ads/1\"\n",
    "evaluationFilePath = \"../CSV Files/Clusters Evaluation results/GOLD vs Clusters - SBERT.xlsx\"\n",
    "k = 19 # the value for the K-Means\n",
    "modelFileName = \"SbertSentences.txt\"\n",
    "clusterExcelSheetName = \"clusteredArgument- Sbert - \"\n",
    "#defining the model\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device=\"cuda\")\n",
    "\n",
    "## Reading the gold standards and vectorizing them\n",
    "df_ref = pd.read_excel(\"../CSV Files/Goldstandards/AD/Goldstandards.xlsx\")\n",
    "listRef = df_ref['Gold standard']\n",
    "refs = model.encode(listRef)\n",
    "\n",
    "sentences = getSentenceUsingStructringWords (dataFilePath,False)\n",
    "corpus_embeddings = model.encode(sentences, show_progress_bar =True, device=\"cuda\")\n",
    "\n",
    "iterateAndAssignGoldStandard(corpus_embeddings,sentences,refs,\"Sbert With Centriods.xlsx\",False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get the avg words per sentence for each method"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from Utility.ThesisUtility import getSentcesAndTokens, avg_words_in_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of NormalSentences 34692\n",
      "number of Segments 33747\n",
      "Avg number of words NormalSentences 20.970973135016717\n",
      "Avg number of words Segments 21.447891664444246\n"
     ]
    }
   ],
   "source": [
    "from Utility.ThesisUtility import getSentcesAndTokens, avg_words_in_list,getSentenceUsingStructringWords\n",
    "\n",
    "dataFilePath = \"../Dataset/Essays_all/TeachersMerged T1+T2\"\n",
    "\n",
    "NormalSentences = getSentcesAndTokens(dataFilePath,False)\n",
    "Segments = getSentenceUsingStructringWords (dataFilePath,False)\n",
    "\n",
    "avgNomrmalSentences = avg_words_in_list(NormalSentences)\n",
    "avgSegments = avg_words_in_list(Segments)\n",
    "\n",
    "print(\"number of NormalSentences\", len(NormalSentences))\n",
    "print(\"number of Segments\", len(Segments))\n",
    "\n",
    "print(\"Avg number of words NormalSentences\", avgNomrmalSentences)\n",
    "print(\"Avg number of words Segments\", avgSegments)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate the linear Kappa between the 1st and the 2nd annotator."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa -  0.7327630097329116\n",
      "Pearson -  (0.7844785974433292, 2.9838106356851154e-50)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "\n",
    "df = pd.read_excel(\"C:/Users/Ahmed/Downloads/nlp/ToBeAnnotatedSentences - merged.xlsx\")\n",
    "y1 = df.query(f\"Final_1st != {-1}\")['Final_1st']\n",
    "y2 = df.query(f\"Final_2nd != {-1}\")['Final_2nd']\n",
    "\n",
    "print (\"Kappa - \", cohen_kappa_score(y1[0:235],y2[0:235],weights=\"linear\"))\n",
    "print (\"Pearson - \", scipy.stats.pearsonr(y1[0:235], y2[0:235]))\n",
    "cm  = confusion_matrix(y1[0:235], y2[0:235])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Graph the 2 sets of annotations in form of a confusion matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Change figure size and increase dpi for better resolution\n",
    "plt.figure(figsize=(20,10), dpi=1000)\n",
    "# Scale up the size of all text\n",
    "sns.set(font_scale = 1.1)\n",
    "\n",
    "# Plot Confusion Matrix using Seaborn heatmap()\n",
    "# Parameters:\n",
    "# first param - confusion matrix in array format\n",
    "# annot = True: show the numbers in each heatmap cell\n",
    "# fmt = 'd': show numbers as integers.\n",
    "ax = sns.heatmap(cm, annot=True, fmt='d', )\n",
    "\n",
    "# set x-axis label and ticks.\n",
    "ax.set_xlabel(\"1st annotator\", fontsize=14, labelpad=20)\n",
    "\n",
    "# set y-axis label and ticks\n",
    "ax.set_ylabel(\"2nd annotator\", fontsize=14, labelpad=20)\n",
    "\n",
    "# set plot title\n",
    "ax.set_title(\"Confusion Matrix between 1st and 2nd annotator\", fontsize=14, pad=20)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}